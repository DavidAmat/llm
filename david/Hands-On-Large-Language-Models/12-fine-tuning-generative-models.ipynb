{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12 - Fine Tuning Generation Models\n",
    "\n",
    "- Supervised Fine Tuning (SFT)\n",
    "- Preference Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<img src=\"imgs/nertags.png\" alt=\"Patching\" width=\"500\" height=\"200\">\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Three LLM Training Steps: Pretraining, Supervised Fine-Tuning, and Preference Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Language modeling\n",
    "\n",
    "The first step in creating a high-quality LLM is to pretrain it on one or more massive text datasets\n",
    "\n",
    "It attempts to predict the next token\n",
    "\n",
    "This produces a base model, also commonly referred to as a pretrained or foundation model.\n",
    "\n",
    "### 2. Fine-tuning 1 (supervised fine-tuning)\n",
    "\n",
    "LLMs are more useful if they respond well to instructions\n",
    "\n",
    "With supervised fine-tuning (SFT), we can adapt the base model to follow instructions. \n",
    "\n",
    "the parameters of the base model are updated to be more in line with our target task\n",
    "\n",
    "Like a pretrained model, it is **trained using next-token prediction but instead of only predicting the next token, it does so based on a user input**\n",
    "\n",
    "SFT can also be used for other tasks, like classification, but is often used to go from a base generative model to an instruction (or chat) generative model.\n",
    "\n",
    "### 3. Fine-tuning 2 (preference tuning)\n",
    "\n",
    "The final step further improves the quality of the model and makes it more aligned with the expected behavior of AI safety or human preferences. \n",
    "\n",
    "Preference tuning is a form of fine-tuning and, as the name implies, aligns the output of the model to our preferences, which are defined by the data that we give it. \n",
    "\n",
    "\n",
    "**In this chapter, we use a base model that was already trained on massive datasets and explore how we can fine-tune it using both fine-tuning strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT)\n",
    "\n",
    "- The most common fine-tuning process is full fine-tuning. The main difference is that we now use a smaller but labeled dataset whereas the pretraining process was done on a large dataset without any labels\n",
    "\n",
    "<img src=\"imgs/sft.png\" alt=\"Patching\" width=\"500\" height=\"200\">\n",
    "\n",
    " To make our LLM follow instructions, we will need **question-response data**.\n",
    "\n",
    " - During full fine-tuning, the model takes the input (instructions) and applies next-token prediction on the output (response) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "Updating all parameters of a model has a large potential of increasing its performance but comes with several disadvantages. \n",
    "- costly\n",
    "- slow training\n",
    "- high storage\n",
    "\n",
    "\n",
    "### Adapters\n",
    "\n",
    "Adapters are a core component of many PEFT-based techniques. The method proposes a set of additional modular components inside the Transformer that can be fine-tuned to improve the model’s performance on a specific task without having to fine-tune all the model weights. \n",
    "\n",
    "PEFT paper showed that fine-tuning 3.6% of the parameters of BERT for a task can yield comparable performance to fine-tuning all the model’s weights\n",
    "\n",
    "<img src=\"imgs/adapters.png\" alt=\"Patching\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Adapters add a small number of weights in certain places in the network that can be fine-tuned efficiently while leaving the majority of model weights frozen.\n",
    "\n",
    " **These Adapters have to be placed in every Transformer block**\n",
    "\n",
    " Find specialized adapters in [AdapterHub](https://adapterhub.ml/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-Rank Adaptation (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to adapters, low-rank adaptation (LoRA) was introduced and is at the time of writing is a widely used and effective technique for PEFT. \n",
    "\n",
    "LoRA is a technique that (like adapters) only requires updating a small set of parameters.\n",
    "\n",
    "<img src=\"imgs/lora.png\" alt=\"Patching\" width=\"300\" height=\"200\">\n",
    "\n",
    "- Like adapters, this subset allows for much quicker fine-tuning since we only need to update a small part of the base model. \n",
    "\n",
    "-  We create this subset of parameters by **approximating large matrices that accompany the original LLM with smaller matrices**. \n",
    "\n",
    "<img src=\"imgs/lorarank.png\" alt=\"Patching\" width=\"300\" height=\"200\">\n",
    "\n",
    "During training, we only need to update these smaller matrices instead of the full weight changes. The updated change matrices (smaller matrices) are then combined with the full (frozen) weights \n",
    "\n",
    "<img src=\"imgs/loratune.png\" alt=\"Patching\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers like “Intrinsic dimensionality explains the effectiveness of language model fine-tuning” demonstrate that language models “have a very low intrinsic dimension\n",
    "\n",
    "This means that we can find small ranks that approximate even the massive matrices of an LLM. \n",
    "\n",
    "A 175B model like GPT-3, for example, would have a weight matrix of **12,288 × 12,288** inside each of its 96 Transformer blocks. **That’s 150 million parameters.**\n",
    "\n",
    " If we can successfully adapt that matrix into rank 8, that would only require two 12,288 × 2 matrices resulting in 197K parameters per block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LoRA we would only train 0.13% of the total weights\n"
     ]
    }
   ],
   "source": [
    "pct = (197 / 150_000) * 100\n",
    "print(f'In LoRA we would only train {pct:.2f}% of the total weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This smaller representation is quite flexible in that you can select which parts of the base model to fine-tune.**  For instance, we can only fine-tune the Query and Value weight matrices in each Transformer layer.\n",
    "\n",
    "#### Quantization: Compressing the model for (more) efficient training\n",
    "\n",
    "We can make LoRA even more efficient by **reducing the memory requirements of the model’s original weights** before projecting them into smaller matrices. \n",
    "\n",
    "The weights of an LLM are numeric values with a given precision\n",
    "\n",
    "<img src=\"imgs/floats.png\" alt=\"Patching\" width=\"600\" height=\"200\">\n",
    "\n",
    "However, if we lower the number of bits we also lower the memory requirements of that model.\n",
    "\n",
    "**With quantization, we aim to lower the number of bits while still accurately representing the original weight values.**\n",
    "\n",
    "Quantizing weights that are close to one another results in the same reconstructed weights thereby removing any differentiating factor.\n",
    "\n",
    "<img src=\"imgs/reconstructweight.png\" alt=\"Patching\" width=\"600\" height=\"200\">\n",
    "\n",
    "### QLORA\n",
    "\n",
    "The authors of QLoRA, a quantized version of LoRA, found a way to go from a higher number of bits to a lower value and vice versa without differentiating too much from the original weights\n",
    "\n",
    "They used **blockwise quantization** to map certain blocks of higher precision values to lower precision values. \n",
    "\n",
    "Instead of directly mapping higher precision to lower precision values, **additional blocks are created that allow for quantizing similar weights**.\n",
    "\n",
    "<img src=\"imgs/qlora.png\" alt=\"Patching\" width=\"600\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice property of neural networks is that their values are generally normally distributed between –1 and 1. \n",
    "\n",
    "**This property allows us to bin the original weights to lower bits based on their relative density**\n",
    "\n",
    "The mapping between weights is more efficient as it takes into account the relative frequency of weights. This also reduces issues with outliers.\n",
    "\n",
    "**Using distribution-aware blocks we can prevent values close to one another from being represented with the same quantized value.**\n",
    "\n",
    "<img src=\"imgs/qloradist.png\" alt=\"Patching\" width=\"500\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a result, we can go from a 16-bit float representation to a measly 4-bit normalized float representation.**\n",
    "\n",
    "*Note that the quantization of LLMs in general is also helpful for inference as quantized LLMs are smaller in size and therefore require less VRAM.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Tuning with QLORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will fine-tune a completely open source and smaller version of Llama, TinyLlama, to follow instructions using the QLoRA procedure.\n",
    "\n",
    "Consider this model a base or pretrained model, one that was trained with language modeling but cannot yet follow instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Instruction Dataset (UltraChat dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chat Template**: To have the LLM follow instructions, we will need to prepare instruction data that follows a chat template.\n",
    "\n",
    "<img src=\"imgs/chattemplate.png\" alt=\"Patching\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UltraChat dataset**: this dataset is a filtered version of the original UltraChat dataset that contains almost 200k conversations between a user and an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d4782c55e5459796e69e1a5971e69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699b7dab010245a89c46feb8f8de05bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fd26f99f6449668a6d412890b1323c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379e117f32d44437b5cdeedf9ef02ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a tokenizer to use its chat template\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\"\n",
    "\n",
    "    # Format answers\n",
    "    chat = example[\"messages\"]\n",
    "    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fa34d4389a4a42a463429875d978e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e8e32546ca4292bdf26c906fc99eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f58219bb7c40c2bea83b62185a6a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e605d07232e4bb5badb1661b902250d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adc0c445df540b38c020d315dc419aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26e3a18f75b46b6ac80ddc15758596c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e05483779c5411db76d91fa63c0d388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/243M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfdca864aa44271beeb911cfd4cd4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/243M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8a97b72b294bd58a71f8d9b64cfeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/80.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fefb58be2b498ab84c113786c9fdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_sft split:   0%|          | 0/207865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e8aaabb5274f42b48c9100297a92d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_sft split:   0%|          | 0/23110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4138ab78e34a22a5d17ede8c45c151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_gen split:   0%|          | 0/256032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c164e774a6f47609229fb34919e4a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_gen split:   0%|          | 0/28304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and format the data using the template TinyLLama is using\n",
    "dataset = (\n",
    "    load_dataset(\"HuggingFaceH4/ultrachat_200k\",  split=\"test_sft\")\n",
    "      .shuffle(seed=42)\n",
    "      .select(range(3_000))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fc308a16664ce783d5f238c31108a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Given the text: Knock, knock. Who’s there? Hike.\n",
      "Can you continue the joke based on the given text material \"Knock, knock. Who’s there? Hike\"?</s>\n",
      "<|assistant|>\n",
      "Sure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold outside!</s>\n",
      "<|user|>\n",
      "Can you tell me another knock-knock joke based on the same text material \"Knock, knock. Who's there? Hike\"?</s>\n",
      "<|assistant|>\n",
      "Of course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here and let's go for a walk!</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of formatted prompt\n",
    "print(dataset[\"text\"][2576])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the text: Knock, knock. Who’s there? Hike.\n",
      "Can you continue the joke based on the given text material \"Knock, knock. Who’s there? Hike\"?\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"prompt\"][2576])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Given the text: Knock, knock. Who’s there? Hike.\\nCan you continue the joke based on the given text material \"Knock, knock. Who’s there? Hike\"?', 'role': 'user'}\n",
      "{'content': \"Sure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold outside!\", 'role': 'assistant'}\n",
      "{'content': 'Can you tell me another knock-knock joke based on the same text material \"Knock, knock. Who\\'s there? Hike\"?', 'role': 'user'}\n",
      "{'content': \"Of course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here and let's go for a walk!\", 'role': 'assistant'}\n"
     ]
    }
   ],
   "source": [
    "for dd in dataset[\"messages\"][2576]:\n",
    "    print(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we can start loading in our model. This is where we apply the Q in QLoRA, namely quantization. We use the bitsandbytes package to compress the pretrained model to a 4-bit representation.\n",
    "\n",
    "In BitsAndBytesConfig, you can define the quantization scheme. We follow the steps used in the original QLoRA paper and load the model in 4-bit (load_in_4bit) with a normalized float representation (bnb_4bit_quant_type) and double quantization (bnb_4bit_use_double_quant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# 4-bit quantization configuration - Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2277227faa9e4d2fab5c63f2f6383bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19afb9416b1543939402262326c39ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c93715a3ad4ca59da2d0dafb9b347d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model to train on the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "\n",
    "    # Leave this out for regular SFT\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0af3dd9fea437087232a25ea353e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb1e115a3104444b9ab584666bdb064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f547474f0bcf4f5da72c82239514cfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27a19ec8cfa40938234945a760e66c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.is_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LoRA Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Scaling\n",
    "    lora_dropout=0.1,  # Dropout for LoRA Layers\n",
    "    r=64,  # Rank\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # Layers to target\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- r: this is the rank of the compressed matrices (recall this from Figure 12-13) Increasing this value will also increase the sizes of compressed matrices leading to less compression and thereby improved representative power. Values typically range between 4 and 64.\n",
    "- lora_alpha: controls the amount of change that is added to the original weights. In essence, it balances the knowledge of the original model with that of the new task. A rule of thumb is to choose a value twice the size of r.\n",
    "- target_modules: controls which layers to target. The LoRA procedure can choose to ignore specific layers, like specific projection layers. This can speed up training but reduce performance and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./training_results\"\n",
    "\n",
    "# Training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **num_train_epochs**: The total number of training rounds. Higher values tend to degrade performance so we generally like to keep this low.\n",
    "- **learning_rate**: Determines the step size at each iteration of weight updates. The authors of QLoRA found that higher learning rates work better for larger models (>33B parameters).\n",
    "- **lr_scheduler_type**: A cosine-based scheduler to adjust the learning rate dynamically. It will linearly increase the learning rate, starting from zero, until it reaches the set value. After that, the learning rate is decayed following the values of a cosine function.\n",
    "- **optim**: The paged optimizers used in the original QLoRA paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/transformers/training_args.py:2027: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0085dc8c393e4aedad9a39cfedb68cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Leave this out for regular SFT\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001bfa9d1c8446849b4e6d3ad9081f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.669, 'grad_norm': 0.27586522698402405, 'learning_rate': 0.00019964928592495045, 'epoch': 0.03}\n",
      "{'loss': 1.4762, 'grad_norm': 0.263204962015152, 'learning_rate': 0.0001985996037070505, 'epoch': 0.05}\n",
      "{'loss': 1.4508, 'grad_norm': 0.1895277500152588, 'learning_rate': 0.0001968583161128631, 'epoch': 0.08}\n",
      "{'loss': 1.488, 'grad_norm': 0.19641973078250885, 'learning_rate': 0.00019443763702374812, 'epoch': 0.11}\n",
      "{'loss': 1.4785, 'grad_norm': 0.19293968379497528, 'learning_rate': 0.0001913545457642601, 'epoch': 0.13}\n",
      "{'loss': 1.3908, 'grad_norm': 0.20264646410942078, 'learning_rate': 0.00018763066800438636, 'epoch': 0.16}\n",
      "{'loss': 1.4949, 'grad_norm': 0.22744004428386688, 'learning_rate': 0.00018329212407100994, 'epoch': 0.19}\n",
      "{'loss': 1.4503, 'grad_norm': 0.19911472499370575, 'learning_rate': 0.000178369345732584, 'epoch': 0.21}\n",
      "{'loss': 1.4273, 'grad_norm': 0.20271213352680206, 'learning_rate': 0.00017289686274214118, 'epoch': 0.24}\n",
      "{'loss': 1.4047, 'grad_norm': 0.22909492254257202, 'learning_rate': 0.00016691306063588583, 'epoch': 0.27}\n",
      "{'loss': 1.4143, 'grad_norm': 0.20257829129695892, 'learning_rate': 0.0001604599114862375, 'epoch': 0.29}\n",
      "{'loss': 1.3772, 'grad_norm': 0.18574774265289307, 'learning_rate': 0.00015358267949789966, 'epoch': 0.32}\n",
      "{'loss': 1.3321, 'grad_norm': 0.19359977543354034, 'learning_rate': 0.00014632960351198618, 'epoch': 0.35}\n",
      "{'loss': 1.4973, 'grad_norm': 0.20577728748321533, 'learning_rate': 0.0001387515586452103, 'epoch': 0.37}\n",
      "{'loss': 1.3465, 'grad_norm': 0.26526716351509094, 'learning_rate': 0.00013090169943749476, 'epoch': 0.4}\n",
      "{'loss': 1.4115, 'grad_norm': 0.20926058292388916, 'learning_rate': 0.00012283508701106557, 'epoch': 0.43}\n",
      "{'loss': 1.4544, 'grad_norm': 0.17107073962688446, 'learning_rate': 0.00011460830285624118, 'epoch': 0.45}\n",
      "{'loss': 1.3246, 'grad_norm': 0.21883808076381683, 'learning_rate': 0.00010627905195293135, 'epoch': 0.48}\n",
      "{'loss': 1.4194, 'grad_norm': 0.1909826248884201, 'learning_rate': 9.790575801166432e-05, 'epoch': 0.51}\n",
      "{'loss': 1.4745, 'grad_norm': 0.1966351568698883, 'learning_rate': 8.954715367323468e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4042, 'grad_norm': 0.19684630632400513, 'learning_rate': 8.126186854142752e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3421, 'grad_norm': 0.195834219455719, 'learning_rate': 7.310801793847344e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3613, 'grad_norm': 0.19288764894008636, 'learning_rate': 6.51427952678185e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3871, 'grad_norm': 0.18304216861724854, 'learning_rate': 5.7422070843492734e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3533, 'grad_norm': 0.19219407439231873, 'learning_rate': 5.000000000000002e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3456, 'grad_norm': 0.19951102137565613, 'learning_rate': 4.2928643231556844e-05, 'epoch': 0.69}\n",
      "{'loss': 1.4659, 'grad_norm': 0.1969127506017685, 'learning_rate': 3.6257601025131026e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4336, 'grad_norm': 0.20671261847019196, 'learning_rate': 3.0033665948663448e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3874, 'grad_norm': 0.20525100827217102, 'learning_rate': 2.4300494434824373e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3764, 'grad_norm': 0.17728424072265625, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3952, 'grad_norm': 0.19482940435409546, 'learning_rate': 1.4463573983949341e-05, 'epoch': 0.83}\n",
      "{'loss': 1.438, 'grad_norm': 0.18023940920829773, 'learning_rate': 1.042882397605871e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3873, 'grad_norm': 0.1932518631219864, 'learning_rate': 7.022351411174866e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3879, 'grad_norm': 0.18788926303386688, 'learning_rate': 4.268050246793276e-06, 'epoch': 0.91}\n",
      "{'loss': 1.3137, 'grad_norm': 0.1875542253255844, 'learning_rate': 2.1852399266194314e-06, 'epoch': 0.93}\n",
      "{'loss': 1.4445, 'grad_norm': 0.18839120864868164, 'learning_rate': 7.885298685522235e-07, 'epoch': 0.96}\n",
      "{'loss': 1.4518, 'grad_norm': 0.19671615958213806, 'learning_rate': 8.771699011416168e-08, 'epoch': 0.99}\n",
      "{'train_runtime': 232.0384, 'train_samples_per_second': 12.929, 'train_steps_per_second': 1.616, 'train_loss': 1.4168811581929526, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.4168811581929526, metrics={'train_runtime': 232.0384, 'train_samples_per_second': 12.929, 'train_steps_per_second': 1.616, 'total_flos': 9994755938844672.0, 'train_loss': 1.4168811581929526, 'epoch': 1.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save QLoRA weights\n",
    "trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Merge Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have trained our QLoRA weights, we still need to combine them with the original weights to use them. \n",
    "\n",
    "**We reload the model in 16 bits, instead of the quantized 4 bits, to merge the weights.**\n",
    "\n",
    "Although the tokenizer was not updated during training, we save it to the same folder as the model for easier access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the adapter with the base model, we can use it with the prompt template that we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Tell me something about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) that can generate human-like language. They are trained on large amounts of data, including text, audio, and video, and are capable of generating complex sentences and phrases that are often difficult to create by humans.\n",
      "\n",
      "LLMs are used in a variety of applications, including natural language processing (NLP), machine translation, and chatbots. They can be used to generate text in different languages, such as English, French, or German, and can also be used to generate images, videos, or other forms of content.\n",
      "\n",
      "One of the most significant applications of LLMs is in the field of natural language generation (NLG). NLG is the process of generating human-like language from a computer program. LLMs can be used to generate text in a variety of fields, including marketing, finance, and medicine.\n",
      "\n",
      "Another application of LLMs is in the field of chatbots. Chatbots are computer programs that can respond to user queries in a conversational manner. LLMs can be used to generate responses to user queries in a natural and human-like manner.\n",
      "\n",
      "Overall, LLMs are a powerful tool that can be used in a variety of applications to generate human-like language. They are becoming increasingly popular in the field of AI and are expected to play a significant role in the future of technology.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use our predefined prompt template\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Run our instruction-tuned model\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate output shows that the model now closely follows our instructions, which is not possible with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# Load the model to train on the GPU\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use our predefined prompt template\n",
    "# prompt = \"\"\"<|user|>\n",
    "# Tell me something about Large Language Models.</s>\n",
    "# <|assistant|>\n",
    "# \"\"\"\n",
    "\n",
    "# # Run our instruction-tuned model\n",
    "# pipe_original = pipeline(task=\"text-generation\", model=original_model, tokenizer=tokenizer)\n",
    "# print(pipe_original(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Level Metrics\n",
    "\n",
    "These classic techniques compare a reference dataset with the generated tokens on a token(set) level. \n",
    "\n",
    "Common word-level metrics include perplexity, ROUGE, BLEU, and BERTScore\n",
    "\n",
    "**Perplexity**: how well a language model predicts a text. Given input text, the model predicts how likely the next token is. With perplexity, we assume a model performs better if it gives the next token a high probability. In other words, the models should not be “perplexed” when presented with a well-written document.\n",
    "\n",
    "**They do not account for consistency, fluency, creativity, or even correctness of the generated text.**\n",
    "\n",
    "### Benchmarks\n",
    "\n",
    "A common method for evaluating generative models on language generation and understanding tasks is on well-known and public benchmarks, such as:\n",
    "\n",
    "- **MMLU**: The Massive Multitask Language Understanding (MMLU) benchmark tests the model on 57 different tasks, including classification, question answering, and sentiment analysis.\n",
    "- **GLUE**: The General Language Understanding Evaluation (GLUE) benchmark consists of language understanding tasks covering a wide degree of difficulty.\n",
    "- **TruthfulQA**: TruthfulQA measures the truthfulness of a model’s generated text.\n",
    "- **GSM8k**: The GSM8k dataset contains grade-school math word problems. It is linguistically diverse and created by human problem writers.\n",
    "- **HellaSwag**: HellaSwag is a challenge dataset for evaluating common-sense inference. It consists of multiple-choice questions that the model needs to answer. It can select one of four answer choices for each question.\n",
    "\n",
    "For coding:\n",
    "- **HumanEval**:  The HumanEval benchmark is used for evaluating generated code based on 164 programming problems.\n",
    "\n",
    "A downside to public benchmarks is that models can be overfitted to these benchmarks to generate the best responses.\n",
    "\n",
    "Moreover, these are still broad benchmarks and might not cover very specific use cases. \n",
    "\n",
    "Lastly, another downside is that some benchmarks require strong GPUs with a long running time (over hours) to compute, which makes iteration difficult.\n",
    "\n",
    "\n",
    "### Leaderboards\n",
    "\n",
    "Whenever a model is released, you will often see it evaluated on several benchmarks to showcase how it performs across the board.\n",
    "\n",
    "A common leaderboard is the [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/), which, at the time of writing, includes six benchmarks, including HellaSwag, MMLU, TruthfulQA, and GSM8k.\n",
    "\n",
    "\n",
    "### Automated Evaluation\n",
    "\n",
    "Part of evaluating a generative output is the quality of its text. For instance, even if two models were to give the same correct answer to a question, the way they derived that answer might be different. \n",
    "\n",
    "Similarly, although two summaries might be similar, one could be significantly shorter than another, which is often important for a good summary.\n",
    "\n",
    "To evaluate the quality of the generated text above the correctness of the final answer, **LLM-as-a-judge** was introduced\n",
    "\n",
    "An interesting variant of this method is **pairwise comparison**. Two different LLMs will generate an answer to a question and a third LLM will be the judge to declare which is better.\n",
    "\n",
    "\n",
    "### Human Evaluation\n",
    "\n",
    "The gold standard of evaluation is generally considered to be human evaluation\n",
    "\n",
    "Even if an LLM scores well on broad benchmarks, it still might not score well on domain-specific tasks.\n",
    "\n",
    "Moreover, benchmarks do not fully capture human preference and all methods discussed before are merely proxies for that.\n",
    "\n",
    "A great example of a human-based evaluation technique is the [Chatbot Arena](https://lmarena.ai/). For instance, if a low-ranked LLM beats a high-ranked LLM, its ranking changes significantly. In chess, this is referred to as the Elo rating system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Tuning / Alignment / RLHF\n",
    "\n",
    "Although our model can now follow instructions, we can further improve its behavior by a final training phase that aligns it to how we expect it to behave in different scenarios. \n",
    "\n",
    "For instance, when asked “What is an LLM?” we might prefer an elaborate answer that describes the internals of an LLM compared to the answer “It is a large language model” without further explanations. \n",
    "\n",
    "**Preference evaluator**: We can ask a person (preference evaluator) to evaluate the quality of that model generation. \n",
    "\n",
    "<img src=\"imgs/pref.png\" alt=\"Patching\" width=\"500\" height=\"200\">\n",
    "\n",
    "- If the score is high, the model is updated to encourage it to generate more like this type of generation.\n",
    "- If the score is low, the model is updated to discourage such generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Preference Evaluation Using Reward Models\n",
    "\n",
    "We need a step before the preference-tuning step, namely to train a reward model.\n",
    "\n",
    "<img src=\"imgs/reward.png\" alt=\"Patching\" width=\"600\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We take a copy of the instruction-tuned model and slightly change it so that instead of generating text, it now outputs a single score.\n",
    "\n",
    "\n",
    "<img src=\"imgs/rewardhead.png\" alt=\"Patching\" width=\"600\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Inputs and Outputs of a Reward Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot directly use the reward model. It needs to first be trained to properly score generations. So let’s get a **preference dataset** \n",
    "\n",
    "\n",
    "### Reward model training dataset\n",
    "\n",
    "One common shape for preference datasets is for a training example to have a prompt, with one accepted generation and one rejected generation.\n",
    "\n",
    "One way to generate preference data is to present a prompt to the LLM and have it generate two different generations.\n",
    "\n",
    "\n",
    "### Reward model training step\n",
    "\n",
    "Now that we have the preference training dataset, we can proceed to train the reward model.\n",
    "\n",
    "A simple step is that we use the reward model to:\n",
    "\n",
    "1. Score the accepted generation\n",
    "2. Score the rejected generation\n",
    "\n",
    "\n",
    "<img src=\"imgs/qualityhead.png\" alt=\"Patching\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final pipeline (3 stages)\n",
    "\n",
    "- Collect preference data\n",
    "- Train a reward model\n",
    "- Use the reward model to fine-tune the LLM (operating as the preference evaluator)\n",
    "\n",
    "\n",
    "<img src=\"imgs/rewardmodel.png\" alt=\"Patching\" width=\"600\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Llama 2, for example, trains two reward models: one that scores helpfulness and another that scores safety**\n",
    "\n",
    "### PPO\n",
    "\n",
    "A common method to fine-tune the LLM with the trained reward model is Proximal Policy Optimization (PPO).\n",
    "\n",
    "PPO is a popular reinforcement technique that optimizes the instruction-tuned LLM by making sure that the LLM does not deviate too much from the expected rewards\n",
    "\n",
    "**A disadvantage of PPO is that it is a complex method that needs to train at least two models, the reward model and the LLM, which can be more costly than perhaps necessary.**\n",
    "\n",
    "\n",
    "### DPO\n",
    "\n",
    "Direct Preference Optimization (DPO) is an alternative to PPO and does away with the reinforcement-based learning procedure\n",
    "\n",
    "Instead of using the reward model to judge the quality of a generation, we let the LLM itself do that. \n",
    "\n",
    "We use a copy of the LLM as the reference model to judge the shift between the reference and trainable model in the quality of the accepted generation and rejected generation.\n",
    "\n",
    "\n",
    "<img src=\"imgs/DPO.png\" alt=\"Patching\" width=\"600\" height=\"300\">\n",
    "\n",
    "By calculating this shift during training, we can optimize the likelihood of accepted generations over rejected generations by tracking the difference in the reference model and the trainable model.\n",
    "\n",
    "\n",
    "### DPO Calculations\n",
    "\n",
    "To calculate this shift and its related scores, the **log probabilities of the rejected generations and accepted generations are extracted from both models**. \n",
    "\n",
    "This process is performed at a token level where the probabilities are combined to calculate the shift between the reference and trainable models:\n",
    "\n",
    "<img src=\"imgs/dposcores.png\" alt=\"Patching\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these scores, we can optimize the parameters of the trainable model to be more confident of generating the accepted generations and less confident of generating the rejected generations.\n",
    "\n",
    "Compared to PPO, the authors found DPO to be more stable during training and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Tuning with DPO\n",
    "\n",
    "We will still be using TinyLlama but this time an instruction-tuned version that was first trained using full fine-tuning and then further aligned with DPO.\n",
    "\n",
    "Compared to our initial instruction-tuned model, this LLM was trained on much larger datasets.\n",
    "\n",
    "In this section, we will demonstrate how you can further align this model using DPO with reward-based datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templating Alignment Data\n",
    "\n",
    "We will use a dataset that for each prompt contains an accepted generation and a rejected generation. \n",
    "\n",
    "This dataset was in part generated by ChatGPT with scores on which output should be accepted and which rejected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\"\n",
    "\n",
    "    # Format answers\n",
    "    system = \"<|system|>\\n\" + example['system'] + \"</s>\\n\"\n",
    "    prompt = \"<|user|>\\n\" + example['input'] + \"</s>\\n<|assistant|>\\n\"\n",
    "    chosen = example['chosen'] + \"</s>\\n\"\n",
    "    rejected = example['rejected'] + \"</s>\\n\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": system + prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aaedeac6cf46e4a30e85f63d487469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a4ce398f6848bab2ff07fc78ecb396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/79.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07f54a3f2a9417d96d1c85144c4d0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply formatting to the dataset and select relatively short answers\n",
    "dpo_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e621f6c2956441ebd5568bc9cdafb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpo_dataset = dpo_dataset.filter(\n",
    "    lambda r:\n",
    "        r[\"status\"] != \"tie\" and\n",
    "        r[\"chosen_score\"] >= 8 and\n",
    "        not r[\"in_gsm8k_train\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527387ac4da649bb99447fac32f8d8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpo_dataset = dpo_dataset.map(format_prompt, remove_columns=dpo_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 5922\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': 'Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One.</s>\\n',\n",
       " 'rejected': ' Sure! Here\\'s a sentence that describes all the data you provided:\\n\\n\"Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes.\"</s>\\n',\n",
       " 'prompt': '<|system|>\\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.</s>\\n<|user|>\\nGenerate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One</s>\\n<|assistant|>\\n'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an AI assistant. You will be given a task. You must generate a detailed and long answer.</s>\n",
      "<|user|>\n",
      "Generate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dpo_dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our base model and load it with the LoRA we created previously. As before, we quantize the model to reduce the necessary VRAM for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization configuration - Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA and base model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"training/TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA tokenizer\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LORA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Prepare LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Scaling\n",
    "    lora_dropout=0.1,  # Dropout for LoRA Layers\n",
    "    r=64,  # Rank\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # Layers to target\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO Trainer\n",
    "\n",
    "For the sake of simplicity, we will use the same training arguments as we did before with one difference. Instead of running for a single epoch (which can take up to two hours), we run for 200 steps instead for illustration purposes.\n",
    "\n",
    "We added the `warmup_ratio` parameter, which increases the learning rate from 0 to the learning_rate value we set for the first 10% of steps.\n",
    "\n",
    "**Warmup**: By maintaining a small learning rate at the start (i.e., warmup period), we allow the model to adjust to the data before applying larger learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOConfig\n",
    "\n",
    "output_dir = \"./training_results_ch12\"\n",
    "\n",
    "# Training arguments\n",
    "training_arguments = DPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=200,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_ratio=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_prompt_length, max_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:411: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecc9766c18048839c76229703af67e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9223f07641624c5b945d36edc6677ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6924, 'grad_norm': 1.8012945652008057, 'learning_rate': 4.5e-06, 'rewards/chosen': -0.0006841827416792512, 'rewards/rejected': -0.002232985571026802, 'rewards/accuracies': 0.25, 'rewards/margins': 0.0015488029457628727, 'logps/rejected': -113.9967041015625, 'logps/chosen': -106.90666198730469, 'logits/rejected': -2.9157962799072266, 'logits/chosen': -2.85026478767395, 'epoch': 0.01}\n",
      "{'loss': 0.6777, 'grad_norm': 2.293241024017334, 'learning_rate': 9.5e-06, 'rewards/chosen': 0.002104945247992873, 'rewards/rejected': -0.02996930107474327, 'rewards/accuracies': 0.4625000059604645, 'rewards/margins': 0.03207424655556679, 'logps/rejected': -158.38888549804688, 'logps/chosen': -125.90263366699219, 'logits/rejected': -3.060880422592163, 'logits/chosen': -2.9251315593719482, 'epoch': 0.03}\n",
      "{'loss': 0.6458, 'grad_norm': 2.260286569595337, 'learning_rate': 9.938441702975689e-06, 'rewards/chosen': 0.006185319274663925, 'rewards/rejected': -0.09871865808963776, 'rewards/accuracies': 0.4625000059604645, 'rewards/margins': 0.10490397363901138, 'logps/rejected': -144.46875, 'logps/chosen': -107.30223083496094, 'logits/rejected': -2.948561668395996, 'logits/chosen': -2.8402533531188965, 'epoch': 0.04}\n",
      "{'loss': 0.6068, 'grad_norm': 1.9898120164871216, 'learning_rate': 9.727592877996585e-06, 'rewards/chosen': -0.012395931407809258, 'rewards/rejected': -0.2253464162349701, 'rewards/accuracies': 0.5375000238418579, 'rewards/margins': 0.21295049786567688, 'logps/rejected': -166.577392578125, 'logps/chosen': -129.8780975341797, 'logits/rejected': -2.9651031494140625, 'logits/chosen': -2.853060483932495, 'epoch': 0.05}\n",
      "{'loss': 0.5953, 'grad_norm': 3.3213324546813965, 'learning_rate': 9.37309853569698e-06, 'rewards/chosen': -0.0649225264787674, 'rewards/rejected': -0.33210617303848267, 'rewards/accuracies': 0.48750001192092896, 'rewards/margins': 0.2671836316585541, 'logps/rejected': -168.80062866210938, 'logps/chosen': -135.2851104736328, 'logits/rejected': -2.972229242324829, 'logits/chosen': -2.848412275314331, 'epoch': 0.07}\n",
      "{'loss': 0.6166, 'grad_norm': 4.453927993774414, 'learning_rate': 8.885729807284855e-06, 'rewards/chosen': -0.10155986249446869, 'rewards/rejected': -0.3497631549835205, 'rewards/accuracies': 0.3499999940395355, 'rewards/margins': 0.248203307390213, 'logps/rejected': -125.97395324707031, 'logps/chosen': -109.17301940917969, 'logits/rejected': -2.979987144470215, 'logits/chosen': -2.905790090560913, 'epoch': 0.08}\n",
      "{'loss': 0.5935, 'grad_norm': 1.2931946516036987, 'learning_rate': 8.345653031794292e-06, 'rewards/chosen': -0.07993119210004807, 'rewards/rejected': -0.4424063265323639, 'rewards/accuracies': 0.375, 'rewards/margins': 0.36247509717941284, 'logps/rejected': -136.95071411132812, 'logps/chosen': -108.6227035522461, 'logits/rejected': -3.0190720558166504, 'logits/chosen': -2.9371650218963623, 'epoch': 0.09}\n",
      "{'loss': 0.5322, 'grad_norm': 1.3552721738815308, 'learning_rate': 7.649596321166024e-06, 'rewards/chosen': -0.10710809379816055, 'rewards/rejected': -0.6790767312049866, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5719686150550842, 'logps/rejected': -168.71290588378906, 'logps/chosen': -135.54232788085938, 'logits/rejected': -2.928328514099121, 'logits/chosen': -2.7924704551696777, 'epoch': 0.11}\n",
      "{'loss': 0.5587, 'grad_norm': 3.728756904602051, 'learning_rate': 6.873032967079562e-06, 'rewards/chosen': -0.136454239487648, 'rewards/rejected': -0.7127782106399536, 'rewards/accuracies': 0.4375, 'rewards/margins': 0.5763238668441772, 'logps/rejected': -175.97970581054688, 'logps/chosen': -116.6473159790039, 'logits/rejected': -3.0401804447174072, 'logits/chosen': -2.862894058227539, 'epoch': 0.12}\n",
      "{'loss': 0.6391, 'grad_norm': 0.9732995629310608, 'learning_rate': 6.039558454088796e-06, 'rewards/chosen': -0.2334011048078537, 'rewards/rejected': -0.8386415243148804, 'rewards/accuracies': 0.44999998807907104, 'rewards/margins': 0.6052404642105103, 'logps/rejected': -183.845947265625, 'logps/chosen': -129.87171936035156, 'logits/rejected': -2.930755138397217, 'logits/chosen': -2.791134834289551, 'epoch': 0.14}\n",
      "{'loss': 0.4967, 'grad_norm': 1.6041170358657837, 'learning_rate': 5.174497483512506e-06, 'rewards/chosen': -0.09623585641384125, 'rewards/rejected': -0.9488916397094727, 'rewards/accuracies': 0.574999988079071, 'rewards/margins': 0.8526557683944702, 'logps/rejected': -199.384033203125, 'logps/chosen': -146.4569091796875, 'logits/rejected': -2.9442739486694336, 'logits/chosen': -2.7818243503570557, 'epoch': 0.15}\n",
      "{'loss': 0.5863, 'grad_norm': 5.481719493865967, 'learning_rate': 4.304134495199675e-06, 'rewards/chosen': -0.19808116555213928, 'rewards/rejected': -0.9655258059501648, 'rewards/accuracies': 0.48750001192092896, 'rewards/margins': 0.7674447298049927, 'logps/rejected': -194.53146362304688, 'logps/chosen': -123.50086975097656, 'logits/rejected': -2.858691930770874, 'logits/chosen': -2.6378836631774902, 'epoch': 0.16}\n",
      "{'loss': 0.6311, 'grad_norm': 2.4188456535339355, 'learning_rate': 3.4549150281252635e-06, 'rewards/chosen': -0.20010924339294434, 'rewards/rejected': -0.6929270625114441, 'rewards/accuracies': 0.4124999940395355, 'rewards/margins': 0.49281787872314453, 'logps/rejected': -146.82635498046875, 'logps/chosen': -123.21418762207031, 'logits/rejected': -2.9529621601104736, 'logits/chosen': -2.896744728088379, 'epoch': 0.18}\n",
      "{'loss': 0.5914, 'grad_norm': 3.7258713245391846, 'learning_rate': 2.6526421860705474e-06, 'rewards/chosen': -0.15546901524066925, 'rewards/rejected': -0.8145966529846191, 'rewards/accuracies': 0.42500001192092896, 'rewards/margins': 0.6591275930404663, 'logps/rejected': -140.57229614257812, 'logps/chosen': -127.70418548583984, 'logits/rejected': -3.0298798084259033, 'logits/chosen': -2.94179105758667, 'epoch': 0.19}\n",
      "{'loss': 0.5782, 'grad_norm': 0.6953769326210022, 'learning_rate': 1.9216926233717087e-06, 'rewards/chosen': -0.08705653995275497, 'rewards/rejected': -0.8121799230575562, 'rewards/accuracies': 0.4000000059604645, 'rewards/margins': 0.725123405456543, 'logps/rejected': -141.37307739257812, 'logps/chosen': -81.27275848388672, 'logits/rejected': -2.9032835960388184, 'logits/chosen': -2.780395030975342, 'epoch': 0.2}\n",
      "{'loss': 0.5915, 'grad_norm': 2.0583744049072266, 'learning_rate': 1.2842758726130283e-06, 'rewards/chosen': -0.10797333717346191, 'rewards/rejected': -0.7376341819763184, 'rewards/accuracies': 0.42500001192092896, 'rewards/margins': 0.629660964012146, 'logps/rejected': -149.20135498046875, 'logps/chosen': -108.14622497558594, 'logits/rejected': -2.931851625442505, 'logits/chosen': -2.824251651763916, 'epoch': 0.22}\n",
      "{'loss': 0.6068, 'grad_norm': 2.1501526832580566, 'learning_rate': 7.597595192178702e-07, 'rewards/chosen': -0.08173082023859024, 'rewards/rejected': -0.6523762941360474, 'rewards/accuracies': 0.3499999940395355, 'rewards/margins': 0.5706454515457153, 'logps/rejected': -129.8228759765625, 'logps/chosen': -100.89637756347656, 'logits/rejected': -2.925410032272339, 'logits/chosen': -2.8934428691864014, 'epoch': 0.23}\n",
      "{'loss': 0.6278, 'grad_norm': 5.158164978027344, 'learning_rate': 3.6408072716606346e-07, 'rewards/chosen': -0.21164651215076447, 'rewards/rejected': -0.8125228881835938, 'rewards/accuracies': 0.4000000059604645, 'rewards/margins': 0.6008763313293457, 'logps/rejected': -153.33175659179688, 'logps/chosen': -132.33621215820312, 'logits/rejected': -2.9378466606140137, 'logits/chosen': -2.906102180480957, 'epoch': 0.24}\n",
      "{'loss': 0.6701, 'grad_norm': 6.146705150604248, 'learning_rate': 1.0926199633097156e-07, 'rewards/chosen': -0.1342434138059616, 'rewards/rejected': -0.4521285593509674, 'rewards/accuracies': 0.3375000059604645, 'rewards/margins': 0.3178851306438446, 'logps/rejected': -124.234619140625, 'logps/chosen': -101.573486328125, 'logits/rejected': -2.823815107345581, 'logits/chosen': -2.780583143234253, 'epoch': 0.26}\n",
      "{'loss': 0.5569, 'grad_norm': 2.8229198455810547, 'learning_rate': 3.0458649045211897e-09, 'rewards/chosen': -0.03763740137219429, 'rewards/rejected': -0.7957924604415894, 'rewards/accuracies': 0.4625000059604645, 'rewards/margins': 0.7581549882888794, 'logps/rejected': -162.24261474609375, 'logps/chosen': -107.9720458984375, 'logits/rejected': -2.9664459228515625, 'logits/chosen': -2.8197214603424072, 'epoch': 0.27}\n",
      "{'train_runtime': 224.7541, 'train_samples_per_second': 7.119, 'train_steps_per_second': 0.89, 'train_loss': 0.6047369384765625, 'epoch': 0.27}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.6047369384765625, metrics={'train_runtime': 224.7541, 'train_samples_per_second': 7.119, 'train_steps_per_second': 0.89, 'total_flos': 0.0, 'train_loss': 0.6047369384765625, 'epoch': 0.2701789935832489})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "# Create DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dpo_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    beta=0.1,\n",
    "    max_prompt_length=512,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "# Fine-tune model with DPO\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapter\n",
    "dpo_trainer.model.save_pretrained(\"training/TinyLlama-1.1B-dpo-qlora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a second adapter. To merge both adapters, we iteratively merge the adapters with the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Merge LoRA and base model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"training/TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sft_model = model.merge_and_unload()\n",
    "\n",
    "# Merge DPO LoRA and SFT model\n",
    "dpo_model = PeftModel.from_pretrained(\n",
    "    sft_model,\n",
    "    \"training/TinyLlama-1.1B-dpo-qlora\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "dpo_model = dpo_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combination of SFT+DPO is a great way to first fine-tune your model to perform basic chatting and then align its answers with human preference. However, it does come at a cost since we need to perform two training loops and potentially tweak the parameters in two processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the release of DPO, new methods of aligning preferences have been developed. Of note is **Odds Ratio Preference Optimization (ORPO), a process that combines SFT and DPO into a single training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Tell me something about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) that can generate human-like language. They are trained on large amounts of data, including text, audio, and video, and are capable of generating complex sentences and phrases that are often difficult to create by humans.\n",
      "\n",
      "LLMs are used in a variety of applications, including natural language processing (NLP), machine translation, and chatbots. They can be used to generate text in different languages, such as English, French, or German, and can also be used to generate images, videos, or other forms of content.\n",
      "\n",
      "One of the most significant applications of LLMs is in the field of natural language generation (NLG). NLG is the process of generating human-like language from a computer program. LLMs can be used to generate text in a variety of fields, including marketing, finance, and medicine.\n",
      "\n",
      "Another application of LLMs is in the field of chatbots. Chatbots are computer programs that can respond to user queries in a conversational manner. LLMs can be used to generate responses to user queries in a natural and human-like manner.\n",
      "\n",
      "Overall, LLMs are a powerful tool that can be used in a variety of applications to generate human-like language. They are becoming increasingly popular in the field of AI and are expected to play a significant role in the future of technology.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use our predefined prompt template\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Run our instruction-tuned model\n",
    "pipe = pipeline(task=\"text-generation\", model=dpo_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_hollm",
   "language": "python",
   "name": "kr_hollm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
