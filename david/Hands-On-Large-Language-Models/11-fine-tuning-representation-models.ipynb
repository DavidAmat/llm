{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForMaskedLM\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub==0.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import sample_dataset\n",
    "from setfit import SetFitModel\n",
    "from setfit import TrainingArguments as SetFitTrainingArguments\n",
    "from setfit import Trainer as SetFitTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will go through several methods and applications for fine-tuning BERT models\n",
    "\n",
    "So far we have used frozen models to do classification:\n",
    "- Using task-specific models trained on classification tasks\n",
    "- Using an embedding model to train a LR on top to classify\n",
    "\n",
    "\n",
    "<img src=\"imgs/pretrainedmodels.png\" alt=\"Hugging Face\" height=400 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this section, we will take a similar approach but allow both the model and the classification head to be updated during training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a BERT and a Classification Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated below, we will use a pretrained BERT model and add a neural network as a classification head, both of which will be fine-tuned for classification.\n",
    "\n",
    "<img src=\"imgs/tunebert.png\" alt=\"Hugging Face\" height=400 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning a Pretrained BERT Model\n",
    "\n",
    "We will be using the same dataset we used in Chapter 4 to fine-tune our model, namely the Rotten Tomatoes dataset, which contains 5,331 positive and 5,331 negative movie reviews from Rotten Tomatoes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and splits\n",
    "tomatoes = load_dataset(\"rotten_tomatoes\")\n",
    "train_data, test_data = tomatoes[\"train\"], tomatoes[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our classification task is to select the underlying model we want to use. \n",
    "\n",
    "We use \"bert-base-cased\", which was pretrained on the English Wikipedia as well as a large dataset consisting of unpublished books\n",
    "\n",
    "We define the number of labels that we want to predict beforehand. This is necessary to create the feedforward neural network that is applied on top of our pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Model and Tokenizer\n",
    "model_id = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad to the longest sequence in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "   \"\"\"Tokenize input data\"\"\"\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0e598865404d68823d5c096e57f395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize train/test data\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1103, 2067, 1110]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0][\"input_ids\"][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate F1 score\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    load_f1 = evaluate.load(\"f1\")\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With compute_metrics we can define any number of metrics that we are interested in and that can be printed out or logged during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=1,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer which executes the training process\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Logits: tensor([[0.5500, 0.2029],\n",
      "        [0.7816, 0.3994],\n",
      "        [0.8093, 0.2740],\n",
      "        [0.7848, 0.2894]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure the model is on the same device as the data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Create a dummy batch with tokenized inputs\n",
    "dummy_input = torch.randint(0, tokenizer.vocab_size, (4, 10)).to(device)  # Batch of 4 sequences, each of length 10\n",
    "dummy_labels = torch.tensor([0, 1, 0, 1]).to(device)  # Binary classification labels\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_ids=dummy_input, labels=dummy_labels)\n",
    "\n",
    "# Inspect the output\n",
    "print(\"Loss:\", output.loss)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c504382f951e4108b656788da6c33600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.419, 'grad_norm': 7.954373359680176, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n",
      "{'train_runtime': 19.1303, 'train_samples_per_second': 445.889, 'train_steps_per_second': 27.914, 'train_loss': 0.413470564710067, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.413470564710067, metrics={'train_runtime': 19.1303, 'train_samples_per_second': 445.889, 'train_steps_per_second': 27.914, 'total_flos': 227605451772240.0, 'train_loss': 0.413470564710067, 'epoch': 1.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0f56448fc44e82911cdeb31bc30647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908d84ff8dd945ad9cb5f566e4d5f10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.37388619780540466,\n",
       " 'eval_f1': 0.8382213812677389,\n",
       " 'eval_runtime': 1.6045,\n",
       " 'eval_samples_per_second': 664.364,\n",
       " 'eval_steps_per_second': 41.756,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will freeze the main BERT model and allow only updates to pass through the classification head. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Model and Tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# Print layer names\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 (0â€“11) encoder blocks consisting of attention heads, dense networks, and layer normalization. \n",
    "\n",
    "<img src=\"imgs/bertarch.png\" alt=\"Hugging Face\" height=400 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we want frozen layers to be followed by trainable layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "\n",
    "     # Trainable classification head\n",
    "     if name.startswith(\"classifier\"):\n",
    "        param.requires_grad = True\n",
    "\n",
    "      # Freeze everything else\n",
    "     else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: bert.embeddings.word_embeddings.weight ----- False\n",
      "Parameter: bert.embeddings.position_embeddings.weight ----- False\n",
      "Parameter: bert.embeddings.token_type_embeddings.weight ----- False\n",
      "Parameter: bert.embeddings.LayerNorm.weight ----- False\n",
      "Parameter: bert.embeddings.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.pooler.dense.weight ----- False\n",
      "Parameter: bert.pooler.dense.bias ----- False\n",
      "Parameter: classifier.weight ----- True\n",
      "Parameter: classifier.bias ----- True\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "# We can check whether the model was correctly updated\n",
    "for name, param in model.named_parameters():\n",
    "     print(f\"Parameter: {name} ----- {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef7e991aeea45e79f81711f6d8cd935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7015, 'grad_norm': 3.9625017642974854, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n",
      "{'train_runtime': 5.21, 'train_samples_per_second': 1637.248, 'train_steps_per_second': 102.496, 'train_loss': 0.7005022348982565, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.7005022348982565, metrics={'train_runtime': 5.21, 'train_samples_per_second': 1637.248, 'train_steps_per_second': 102.496, 'total_flos': 227605451772240.0, 'train_loss': 0.7005022348982565, 'epoch': 1.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer which executes the training process\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803b9bfc91cf4b87b1777053e13314a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6876858472824097,\n",
       " 'eval_f1': 0.6636971046770601,\n",
       " 'eval_runtime': 1.2351,\n",
       " 'eval_samples_per_second': 863.059,\n",
       " 'eval_steps_per_second': 54.245,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing up until a given block of Encoders (1-5 frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_id = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Encoder block 10 starts at index 165 and\n",
    "# we freeze everything before that block\n",
    "for index, (name, param) in enumerate(model.named_parameters()):\n",
    "    if index < 165:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e089f2cebbc40aca0f0f3430c219803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4739, 'grad_norm': 2.742690086364746, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n",
      "{'train_runtime': 6.7426, 'train_samples_per_second': 1265.095, 'train_steps_per_second': 79.198, 'train_loss': 0.4699943199586333, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401f93edfff4f55b09678eed6e65485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.41191136837005615,\n",
       " 'eval_f1': 0.8180076628352491,\n",
       " 'eval_runtime': 1.1766,\n",
       " 'eval_samples_per_second': 905.988,\n",
       " 'eval_steps_per_second': 56.943,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer which executes the training process\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It demonstrates that although we generally want to train as many layers as possible, you can get away with training less if you do not have the necessary computing power.\n",
    "\n",
    " If we iteratively do this and plot the F1 evolution:\n",
    "\n",
    " <img src=\"imgs/frozen.png\" alt=\"Hugging Face\" height=400 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot classification is a technique within supervised classification where you have a classifier learn target labels based on only a few labeled examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SetFit: Efficient Fine-Tuning with Few Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It generate high-quality textual representations that are updated during training. \n",
    "\n",
    "Only a few labeled examples are needed for this framework to be competitive with fine-tuning a BERT-like model on a large, labeled dataset as we explored in the previous example.\n",
    "\n",
    "1. Sampling training data\n",
    "    - Based on in-class and out-class selection of labeled data it generates positive (similar) and negative (dissimilar) pairs of sentences\n",
    "\n",
    "2. Fine-tuning embeddings\n",
    "    - Fine-tuning a pretrained embedding model based on the previously generated training data\n",
    "\n",
    "3. Training a classifier\n",
    "    - Create a classification head on top of the embedding model and train it using the previously generated training data\n",
    "\n",
    "\n",
    "### Generate Training Data\n",
    "\n",
    "Before fine-tuning an embedding model, we need to generate training data. The model assumes the training data to be samples of positive (similar) and negative (dissimilar) pairs of sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, for example, we have the training dataset in Figure 11-9 that classifies text into two categories: text about programming languages, and text about pets.\n",
    "\n",
    "In step 1, SetFit handles this problem by generating the necessary data based on in-class and out-class selection\n",
    "\n",
    " when we have 16 sentences about sports, we can create 16 * (16 â€“ 1) / 2 = 120 pairs that we label as positive pairs. We can use this process to generate negative pairs by collecting pairs from different classes.\n",
    "\n",
    " <img src=\"imgs/dataset.png\" alt=\"Hugging Face\" height=200 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 2, we can use the generated sentence pairs to fine-tune the embedding model. This leverages a method called contrastive learning to fine-tune a pretrained BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we generated these pairs in the previous step, we can use them to fine-tune a SentenceTransformers model. \n",
    "\n",
    " <img src=\"imgs/ftsentenceemb.png\" alt=\"Hugging Face\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of fine-tuning this embedding model is that it can create embeddings that are tuned to the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 3, we generate embeddings for all sentences and use those as the input of a classifier.\n",
    "\n",
    " <img src=\"imgs/step3.png\" alt=\"Hugging Face\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All STEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " <img src=\"imgs/steps.png\" alt=\"Hugging Face\" height=400 width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/b_hollm/lib/python3.10/site-packages/setfit/data.py:154: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.apply(lambda x: x.sample(min(num_samples, len(x)), random_state=seed))\n"
     ]
    }
   ],
   "source": [
    "# We simulate a few-shot setting by sampling 16 examples per class\n",
    "sampled_train_data = sample_dataset(tomatoes[\"train\"], num_samples=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 32\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 16, 0: 16})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(sampled_train_data[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since this is a few-shot setting, we will only sample 16 examples per class. With two classes, we will only have 32 documents to train on compared to the 8,500 movie reviews we used before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sampling the data, we choose a pretrained SentenceTransformer model to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained SentenceTransformer model\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, a logistic regression model is chosen as the classifier to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did with Hugging Face Transformers, we can use the trainer to define and play around with relevant parameters. For example, we set the num_epochs to 3 so that contrastive learning will be performed for three epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ee8e1054fc47ac81dfdd3cae298e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = SetFitTrainingArguments(\n",
    "    num_epochs=3, # The number of epochs to use for contrastive learning\n",
    "    num_iterations=20  # The number of text pairs to generate\n",
    ")\n",
    "args.eval_strategy = args.evaluation_strategy\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=sampled_train_data,\n",
    "    eval_dataset=test_data,\n",
    "    metric=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 1280\n",
      "  Batch size = 16\n",
      "  Num epochs = 3\n",
      "  Total optimization steps = 240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb40c4c7ce04c38b5243aeb4bdda6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0901cab88c4d9e8a6625134e8474b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2271, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.01}\n",
      "{'embedding_loss': 0.0016, 'learning_rate': 1.7592592592592595e-05, 'epoch': 0.62}\n",
      "{'embedding_loss': 0.0003, 'learning_rate': 1.2962962962962964e-05, 'epoch': 1.25}\n",
      "{'embedding_loss': 0.0002, 'learning_rate': 8.333333333333334e-06, 'epoch': 1.88}\n",
      "{'embedding_loss': 0.0002, 'learning_rate': 3.7037037037037037e-06, 'epoch': 2.5}\n",
      "{'train_runtime': 12.385, 'train_samples_per_second': 310.052, 'train_steps_per_second': 19.378, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8373764600179695}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on our test data\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued Pretraining with Masked Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained model is often trained on very general data, like Wikipedia pages, and might not be tuned to your domain-specific words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/ft.png\" alt=\"Hugging Face\" height=400 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of adopting this two-step approach, we can squeeze another step between them, namely continue pretraining an already pretrained BERT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we can simply continue training the BERT model using masked language modeling (MLM) but instead use data from our domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is like going from a general BERT model to a BioBERT model specialized for the medical domain, to a fine-tuned BioBERT model to classify medication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will update the subword representations to be more tuned toward words it would not have seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/continuedpretrain.png\" alt=\"Hugging Face\" height=400 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having to pretrain an entire model from scratch, we can simply continue pretraining before fine-tuning it for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this example, we will demonstrate how to apply step 2 and continue pretraining an already pretrained BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model for Masked Language Modeling (MLM)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca0408e9b9548ef9ac780d056465cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf73c7b205f947c9b8cba59557fee25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_train = tokenized_train.remove_columns(\"label\")\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = tokenized_test.remove_columns(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instead, we will have a DataCollator that will perform the masking of tokens for us.**\n",
    "\n",
    "- token masking (15% tokens randomly remove) (CONVERGES FASTER, BUT THE MODEL LEARNS LESS)\n",
    "- whole-word masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Masking Tokens\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=10,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32a4787db2d4cdd9b49e998dcf02f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5959, 'grad_norm': 13.989809036254883, 'learning_rate': 1.812734082397004e-05, 'epoch': 0.94}\n",
      "{'loss': 2.3715, 'grad_norm': 17.446483612060547, 'learning_rate': 1.6254681647940076e-05, 'epoch': 1.87}\n",
      "{'loss': 2.3091, 'grad_norm': 23.203062057495117, 'learning_rate': 1.4382022471910113e-05, 'epoch': 2.81}\n",
      "{'loss': 2.1879, 'grad_norm': 13.093008041381836, 'learning_rate': 1.250936329588015e-05, 'epoch': 3.75}\n",
      "{'loss': 2.151, 'grad_norm': 14.838418006896973, 'learning_rate': 1.0636704119850187e-05, 'epoch': 4.68}\n",
      "{'loss': 2.088, 'grad_norm': 22.59937858581543, 'learning_rate': 8.764044943820226e-06, 'epoch': 5.62}\n",
      "{'loss': 2.0599, 'grad_norm': 18.066375732421875, 'learning_rate': 6.891385767790263e-06, 'epoch': 6.55}\n",
      "{'loss': 1.9871, 'grad_norm': 13.970683097839355, 'learning_rate': 5.0187265917603005e-06, 'epoch': 7.49}\n",
      "{'loss': 1.9874, 'grad_norm': 18.521894454956055, 'learning_rate': 3.146067415730337e-06, 'epoch': 8.43}\n",
      "{'loss': 1.9633, 'grad_norm': 14.674639701843262, 'learning_rate': 1.2734082397003748e-06, 'epoch': 9.36}\n",
      "{'train_runtime': 213.0684, 'train_samples_per_second': 400.341, 'train_steps_per_second': 25.062, 'train_loss': 2.1567308747366574, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Save pre-trained tokenizer\n",
    "tokenizer.save_pretrained(\"mlm\")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save updated model\n",
    "model.save_pretrained(\"mlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer is not updated during training so there is no need to save it after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate its performance we would normally fine-tune the model on a variety of tasks. For our purposes, however, we can run some masking tasks to see if it has learned from its continued training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> What a horrible idea!\n",
      ">>> What a horrible dream!\n",
      ">>> What a horrible thing!\n",
      ">>> What a horrible day!\n",
      ">>> What a horrible thought!\n"
     ]
    }
   ],
   "source": [
    "# Load and create predictions\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "preds = mask_filler(\"What a horrible [MASK]!\")\n",
    "\n",
    "# Print results\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output demonstrates concepts like â€œidea,â€ â€œdream,â€ and â€œday,â€ which definitely make sense. Next, letâ€™s see what our updated model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> What a horrible movie!\n",
      ">>> What a horrible film!\n",
      ">>> What a horrible mess!\n",
      ">>> What a horrible story!\n",
      ">>> What a horrible comedy!\n"
     ]
    }
   ],
   "source": [
    "# Load and create predictions\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"mlm\")\n",
    "preds = mask_filler(\"What a horrible [MASK]!\")\n",
    "\n",
    "# Print results\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A horrible movie, film, mess, etc. clearly shows us that the model is more biased toward the data that we fed it compared to the pretrained model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to fine-tune this model on the classification task that we did at the beginning of this chapter. Simply load the model as follows and you are good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune for classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mlm\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mlm\")\n",
    "\n",
    "# Repeat the same process as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning the pretrained BERT model follows a similar architecture akin to what we observed with document classification. However, there is a fundamental shift in the classification approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Rather than relying on the aggregation or pooling of token embeddings, the model now makes predictions for individual tokens in a sequence. \n",
    "\n",
    " # <img src=\"imgs/ner.png\" alt=\"Patching\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7534115d4104469782db41322e2155de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a0cd11100f42eeb7335bc71c5dd548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce0747575054c0096a72d44ff11f45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c060192a704ab49da8d6f1ce9a025f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0077d810e94f49f9b6b0fdcb11eba193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c1144e90514b9db1f99e912ccd8676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The CoNLL-2003 dataset for NER\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '848',\n",
       " 'tokens': ['Dean',\n",
       "  'Palmer',\n",
       "  'hit',\n",
       "  'his',\n",
       "  '30th',\n",
       "  'homer',\n",
       "  'for',\n",
       "  'the',\n",
       "  'Rangers',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 22, 38, 29, 16, 21, 15, 12, 23, 7],\n",
       " 'chunk_tags': [11, 12, 21, 11, 12, 12, 13, 11, 12, 0],\n",
       " 'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][848]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-MISC': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {\n",
    "    'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4,\n",
    "    'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8\n",
    "}\n",
    "id2label = {index: label for label, index in label2id.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at Datasets with NER for Restaurants and Dish: https://huggingface.co/datasets/tner/mit_restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These entities correspond to specific categories: a person (PER), organization (ORG), location (LOC), miscellaneous entities (MISC), and no entity (O). Note that these entities are prefixed with either a B (beginning) or an I (inside). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If two tokens that follow each other are part of the same phrase, then the start of that phrase is indicated with B, which is followed by an I to show that they belong to each other and are not independent entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/nertags.png\" alt=\"Patching\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is preprocessed and split up into words but not yet tokens. To do so, we will tokenize it further with the tokenizer of the pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Dean',\n",
       " 'Palmer',\n",
       " 'hit',\n",
       " 'his',\n",
       " '30th',\n",
       " 'home',\n",
       " '##r',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Rangers',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split individual tokens into sub-tokens\n",
    "token_ids = tokenizer(example[\"tokens\"], is_split_into_words=True)[\"input_ids\"]\n",
    "sub_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "sub_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer added the [CLS] and [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a bit of a problem for us since we have labeled data at the word level but not at the token level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be resolved by aligning the labels with their subtoken counterparts during tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s consider the word 'Maarten', which has the label B-PER to signal that this is a person. If we pass that word through the tokenizer, it splits the word up into the tokens 'Ma', '##arte', and '##n'. We cannot use the B-PER entity for all tokens as that would signal that the three tokens are all independent people. Whenever an entity is split into tokens, the first token should have B (for beginning) and the following should be I (for inner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a963d3d979408ea3de8b3b7aeda2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c560fbc604814257af0ee3a633bb2649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cea0930f4f84bb4898cf228576fe01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def align_labels(examples):\n",
    "    token_ids = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = examples[\"ner_tags\"]\n",
    "\n",
    "    updated_labels = []\n",
    "    for index, label in enumerate(labels):\n",
    "\n",
    "        # Map tokens to their respective word\n",
    "        word_ids = token_ids.word_ids(batch_index=index)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "\n",
    "            # The start of a new word\n",
    "            if word_idx != previous_word_idx:\n",
    "\n",
    "                previous_word_idx = word_idx\n",
    "                updated_label = -100 if word_idx is None else label[word_idx]\n",
    "                label_ids.append(updated_label)\n",
    "\n",
    "            # Special token is -100\n",
    "            elif word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            else:\n",
    "                updated_label = label[word_idx]\n",
    "                if updated_label % 2 == 1:\n",
    "                    updated_label += 1\n",
    "                label_ids.append(updated_label)\n",
    "\n",
    "        updated_labels.append(label_ids)\n",
    "\n",
    "    token_ids[\"labels\"] = updated_labels\n",
    "    return token_ids\n",
    "\n",
    "tokenized = dataset.map(align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]\n",
      "Updated: [-100, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "# Difference between original and updated labels\n",
    "print(f\"Original: {example['ner_tags']}\")\n",
    "print(f\"Updated: {tokenized['train'][848]['labels']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tokenized and aligned the labels, we can start thinking about defining our **evaluation metrics.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66877544c0ea4fe3b0b03f2d44ee9fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load sequential evaluation\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Create predictions\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=2)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Document-level iteration\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "\n",
    "      # token-level iteration\n",
    "      for token_prediction, token_label in zip(prediction, label):\n",
    "\n",
    "        # We ignore special tokens\n",
    "        if token_label != -100:\n",
    "          true_predictions.append([id2label[token_prediction]])\n",
    "          true_labels.append([id2label[token_label]])\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"f1\": results[\"overall_f1\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning for Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are nearly there. Instead of DataCollatorWithPadding, we need a collator that works with classification on a token level, namely DataCollatorForTokenClassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-classification Data Collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da932bb8859442fea5de02de30422984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/878 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2311, 'grad_norm': 1.5110145807266235, 'learning_rate': 8.610478359908885e-06, 'epoch': 0.57}\n",
      "{'train_runtime': 29.6735, 'train_samples_per_second': 473.183, 'train_steps_per_second': 29.589, 'train_loss': 0.16675895438922023, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=878, training_loss=0.16675895438922023, metrics={'train_runtime': 29.6735, 'train_samples_per_second': 473.183, 'train_steps_per_second': 29.589, 'total_flos': 351240792638148.0, 'train_loss': 0.16675895438922023, 'epoch': 1.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=1,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b41d74e2e94bb58a83633c8b0b1212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.14443840086460114,\n",
       " 'eval_f1': 0.9053670966714444,\n",
       " 'eval_runtime': 2.3446,\n",
       " 'eval_samples_per_second': 1472.721,\n",
       " 'eval_steps_per_second': 92.125,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on our test data\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.98807156,\n",
       "  'index': 4,\n",
       "  'word': 'Ma',\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9644239,\n",
       "  'index': 5,\n",
       "  'word': '##arte',\n",
       "  'start': 13,\n",
       "  'end': 17},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9787199,\n",
       "  'index': 6,\n",
       "  'word': '##n',\n",
       "  'start': 17,\n",
       "  'end': 18}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Save our fine-tuned model\n",
    "trainer.save_model(\"training_ner_model\")\n",
    "\n",
    "# Run inference on the fine-tuned model\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"training_ner_model\",\n",
    ")\n",
    "token_classifier(\"My name is Maarten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_hollm",
   "language": "python",
   "name": "kr_hollm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
