{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing RAG\n",
    "\n",
    "On a separate track, the fast adoption of text generation models led many users to ask the models questions and expect factual answers. And while the models were able to answer fluently and confidently, their answers were not always correct or up-to-date. This problem grew to be known as model “hallucinations,” and one of the leading ways to reduce it is to build systems that can retrieve relevant information and provide it to the LLM to aid it in generating more factual answers. This method, called RAG, is one of the most popular applications of LLMs.\n",
    "\n",
    "Three broad categories of these models are:\n",
    "\n",
    "### Dense Retrieval\n",
    "\n",
    "Dense retrieval systems rely on the concept of embeddings, the same concept we’ve encountered in the previous chapters, and turn the search problem into retrieving the nearest neighbors of the search query\n",
    "\n",
    "### ReRanking\n",
    "\n",
    "Search systems are often pipelines of multiple steps. A reranking language model is one of these steps and is tasked with scoring the relevance of a subset of results against the query. The order of results is then changed based on these scores\n",
    "\n",
    "### RAG\n",
    "\n",
    "The growing LLM capability of text generation led to a new type of search systems that include a model that generates an answer in response to a query.\n",
    "\n",
    "Generative search is a subset of a broader type of category of systems better called RAG systems. These are text generation systems that incorporate search capabilities to reduce hallucinations, increase factuality, and/or ground the generation model on a specific dataset.\n",
    "\n",
    "A RAG system formulates an answer to a question and (preferably) cites its information sources.\n",
    "\n",
    "\n",
    "### General path\n",
    "\n",
    "We chunk a document before proceeding to embed each chunk. Those embedding vectors are then stored in the vector database and are ready for retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Retrieval Example\n",
    "\n",
    "Let’s take a look at a dense retrieval example by using Cohere to search the Wikipedia page for the film Interstellar. In this example, we will do the following:\n",
    "\n",
    "1. Get the text we want to make searchable and apply some light processing to chunk it into sentences.\n",
    "2. Embed the sentences.\n",
    "3. Build the search index.\n",
    "4. Search and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and retrieve a Cohere API key from os.cohere.ai\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown Interstellar wikipedia text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan.\n",
    "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.\n",
    "Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\n",
    "\n",
    "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.\n",
    "Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.\n",
    "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm.\n",
    "Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.\n",
    "Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects.\n",
    "\n",
    "Interstellar premiered on October 26, 2014, in Los Angeles.\n",
    "In the United States, it was first released on film stock, expanding to venues using digital projectors.\n",
    "The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014.\n",
    "It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.\n",
    "It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\n",
    "Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\"\"\"\n",
    "\n",
    "# Split into a list of sentences\n",
    "texts = text.split('.')\n",
    "\n",
    "# Clean up to remove empty spaces and new lines\n",
    "texts = [t.strip(' \\n') for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings\n",
    "response = co.embed(\n",
    "  texts=texts,\n",
    "  input_type=\"search_document\",\n",
    ").embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = np.array(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 4096)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Search Index\n",
    "\n",
    "Before we can search, we need to build a search index. An index stores the embeddings and is optimized to quickly retrieve the nearest neighbors even if we have a very large number of points:\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI Research for efficient similarity search and clustering of dense vectors. \n",
    "\n",
    "- useful in large-scale vector search, such as:\n",
    "    - nearest neighbor search, which is common in applications like recommendation systems, image retrieval, and semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = embeds.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(np.float32(embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, number_of_results=3):\n",
    "  \n",
    "  # 1. Get the query's embedding\n",
    "  query_embed = co.embed(texts=[query], \n",
    "                input_type=\"search_query\",).embeddings[0]\n",
    "\n",
    "  # 2. Retrieve the nearest neighbors\n",
    "  distances , similar_item_ids = index.search(np.float32([query_embed]), number_of_results) \n",
    "\n",
    "  # 3. Format the results\n",
    "  texts_np = np.array(texts) # Convert texts list to numpy for easier indexing\n",
    "  results = pd.DataFrame(data={'texts': texts_np[similar_item_ids[0]], \n",
    "                              'distance': distances[0]})\n",
    "  \n",
    "  # 4. Print and return the results\n",
    "  print(f\"Query:'{query}'\\nNearest neighbors:\")\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:'how precise was the science'\n",
      "Nearest neighbors:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It has also received praise from many astronom...</td>\n",
       "      <td>10757.371094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Caltech theoretical physicist and 2017 Nobel l...</td>\n",
       "      <td>11566.136719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Interstellar uses extensive practical and mini...</td>\n",
       "      <td>11922.841797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts      distance\n",
       "0  It has also received praise from many astronom...  10757.371094\n",
       "1  Caltech theoretical physicist and 2017 Nobel l...  11566.136719\n",
       "2  Interstellar uses extensive practical and mini...  11922.841797"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"how precise was the science\"\n",
    "results = search(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 'It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics'\n",
      "\n",
      "2. 'Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar'\n",
      "\n",
      "3. 'Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,row in results.iterrows():\n",
    "  print(f\"{i+1}. '{row['texts']}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually verify that we will have NOT got the same answer if we had use the classifcal keyword searches. We’ll use the BM25 algorithm, which is one of the leading lexical search methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 173318.35it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = []\n",
    "for passage in tqdm(texts):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6863989535702286"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.idf[\"science\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(query, top_k=3, num_candidates=15):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    print(f\"Top-3 lexical search (BM25) hits\")\n",
    "    for hit in bm25_hits[0:top_k]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: how precise was the science\n",
      "Top-3 lexical search (BM25) hits\n",
      "\t1.789\tInterstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan\n",
      "\t1.373\tCaltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar\n",
      "\t0.000\tIt stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine\n"
     ]
    }
   ],
   "source": [
    "keyword_search(query = \"how precise was the science\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats of Dense Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:'What is the mass of the moon?'\n",
      "Nearest neighbors:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cinematographer Hoyte van Hoytema shot it on 3...</td>\n",
       "      <td>12854.445312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The film had a worldwide gross over $677 milli...</td>\n",
       "      <td>13301.009766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It has also received praise from many astronom...</td>\n",
       "      <td>13332.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts      distance\n",
       "0  Cinematographer Hoyte van Hoytema shot it on 3...  12854.445312\n",
       "1  The film had a worldwide gross over $677 milli...  13301.009766\n",
       "2  It has also received praise from many astronom...  13332.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the mass of the moon?\"\n",
    "results = search(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases like this, one possible heuristic is to set a threshold level—a maximum distance for relevance, for example. A lot of search systems present the user with the best info they can get and leave it up to the user to decide if it’s relevant or not. \n",
    "\n",
    "Tracking the information of whether the user clicked on a result (and were satisfied by it) can improve future versions of the search system.\n",
    "\n",
    "Another caveat of dense retrieval is when a user wants to find an exact match for a specific phrase. \n",
    "\n",
    "**That’s a case that’s perfect for keyword matching. That’s one reason why hybrid search, which includes both semantic search and keyword search, is advised instead of relying solely on dense retrieval.**\n",
    "\n",
    "- Dense retrieval systems also find it challenging to work properly in domains other than the ones that they were trained on\n",
    "\n",
    "- What about questions whose answers span multiple sentences? what is the best way to chunk long texts?\n",
    "\n",
    "## Chunking long texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- indexing one vector per document\n",
    "    - Embedding the document in chunks, embedding those chunks, and then aggregating those chunks into a single vector. \n",
    "    - The usual method of aggregation here is to average those vectors.\n",
    "    - A downside of this approach is that it results in a highly compressed vector that loses a lot of the information in the document.\n",
    "- indexing multiple vectors per document.\n",
    "    - In this approach, we chunk the document into smaller pieces, and embed those chunks. \n",
    "    - Our search index then becomes that of chunk embeddings, not entire document embeddings.\n",
    "    - The chunking approach is better because it has full coverage of the text \n",
    "    - Chunking methods:\n",
    "        - Each sentence is a chunk. The issue here is this could be too granular and the vectors don’t capture enough of the context.\n",
    "        - Each paragraph is a chunk. This is great if the text is made up of short paragraphs. Otherwise, it may be that every 3–8 sentences is a chunk.\n",
    "        - Some chunks derive a lot of their meaning from the text around them. So we can incorporate some context via:\n",
    "            - Adding the title of the document to the chunk.\n",
    "            - **Overlapping Chunks**: Adding some of the text before and after them to the chunk. This way, the chunks can overlap so they include some surrounding text that also appears in adjacent chunks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbor search versus vector databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the query is embedded, we need to find the nearest vectors to it from our text archive\n",
    "\n",
    "<img src=\"imgs/search.png\" alt=\"Cohere logo\" width=\"400\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you scale beyond to the millions of vectors, an optimized approach for retrieval is to rely on **approximate nearest neighbor search** libraries like Annoy or FAISS. \n",
    "\n",
    "Another class of vector retrieval systems are vector databases like Weaviate or Pinecone. \n",
    "- A vector database allows you to add or delete vectors without having to rebuild the index. \n",
    "- They also provide ways to filter your search or customize it in ways beyond merely vector distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning embedding models for dense retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The process for this fine-tuning is to get training data composed of queries and relevant results.\n",
    "\n",
    " “Interstellar premiered on October 26, 2014, in Los Angeles.” Two possible queries where this is a relevant result are:\n",
    "\n",
    "- Relevant query 1: “Interstellar release date”\n",
    "- Relevant query 2: “When did Interstellar premier”\n",
    "\n",
    "The fine-tuning process aims to make the embeddings of these queries close to the embedding of the resulting sentence. It also needs to see negative examples of queries that are not relevant to the sentence, for example:\n",
    "\n",
    "- Irrelevant query: “Interstellar cast”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReRanking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those organizations, an easier way to incorporate language models is as a final step inside their search pipeline\n",
    "\n",
    " This step is tasked with changing the order of the search results based on relevance to the search query. \n",
    "\n",
    "<img src=\"imgs/rerank.png\" alt=\"Cohere logo\" width=\"450\" height=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reranker takes in the search query and a number of search results, and returns the optimal ordering of these documents so the most relevant ones to the query are higher in ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan',\n",
       " 'It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine',\n",
       " 'Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind',\n",
       " 'Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007',\n",
       " 'Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar',\n",
       " 'Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm',\n",
       " 'Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles',\n",
       " 'Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects',\n",
       " 'Interstellar premiered on October 26, 2014, in Los Angeles',\n",
       " 'In the United States, it was first released on film stock, expanding to venues using digital projectors',\n",
       " 'The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014',\n",
       " 'It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight',\n",
       " 'It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics',\n",
       " 'Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time',\n",
       " 'Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RerankResponseResultsItem(document=RerankResponseResultsItemDocument(text='It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics'), index=12, relevance_score=0.16981852),\n",
       " RerankResponseResultsItem(document=RerankResponseResultsItemDocument(text='The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014'), index=10, relevance_score=0.07004896),\n",
       " RerankResponseResultsItem(document=RerankResponseResultsItemDocument(text='Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar'), index=4, relevance_score=0.0043994132)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"how precise was the science\"\n",
    "results = co.rerank(query=query, documents=texts, top_n=3, return_documents=True)\n",
    "results.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this basic example, we passed our reranker all 15 of our documents.\n",
    "\n",
    "However, in production, an index would have thousands or millions of entries, and we need to shortlist, say one hundred or one thousand results and then present those to the reranker. \n",
    "\n",
    "This shortlisting step is called the first stage of the search pipeline.\n",
    "\n",
    "### First-Stage\n",
    "\n",
    "The first-stage retriever can be keyword search, dense retrieval, or better yet—hybrid search that uses both of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_and_reranking_search(query, top_k=3, num_candidates=10):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    print(f\"Top-3 lexical search (BM25) hits\")\n",
    "    for hit in bm25_hits[0:top_k]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    #Add re-ranking\n",
    "    docs = [texts[hit['corpus_id']] for hit in bm25_hits]\n",
    "\n",
    "    print(f\"\\nTop-3 hits by rank-API ({len(bm25_hits)} BM25 hits re-ranked)\")\n",
    "    results = co.rerank(query=query, documents=docs, top_n=top_k, return_documents=True)\n",
    "    for hit in results.results:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit.relevance_score, hit.document.text.replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: how precise was the science\n",
      "Top-3 lexical search (BM25) hits\n",
      "\t1.789\tInterstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan\n",
      "\t1.373\tCaltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar\n",
      "\t0.000\tInterstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects\n",
      "\n",
      "Top-3 hits by rank-API (10 BM25 hits re-ranked)\n",
      "\t0.004\tCaltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar\n",
      "\t0.004\tSet in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind\n",
      "\t0.003\tBrothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007\n"
     ]
    }
   ],
   "source": [
    "keyword_and_reranking_search(query = \"how precise was the science\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open source retrieval and reranking with sentence transformers\n",
    "\n",
    "If you want to locally set up retrieval and reranking on your own machine, then you can use the Sentence Transformers library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How reranking models work\n",
    "\n",
    "One popular way of building LLM search rerankers is to present the query and each result to an LLM working as a cross-encoder. \n",
    "\n",
    "This means that a query and possible result are presented to the model at the same time allowing the model to view both these texts before it assigns a relevance score\n",
    "\n",
    "All of the documents are processed simultaneously as a batch yet each document is evaluated against the query independently. \n",
    "\n",
    "This formulation of search as relevance scoring basically boils down to being a classification problem. Given those inputs, the model outputs a score from 0–1 where 0 is irrelevant and 1 is highly relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation Metrics\n",
    "\n",
    "Evaluating search systems needs three major components: a text archive, a set of queries, and relevance judgments indicating which documents are relevant for each query. \n",
    "\n",
    "\n",
    "To evaluate search systems, we need a test suite including queries and relevance judgments indicating which documents in our archive are relevant for each query.\n",
    "\n",
    "<img src=\"imgs/test_suite.png\" alt=\"Cohere logo\" width=\"450\" height=\"250\"/>\n",
    "\n",
    " Let’s assume we pass query 1 to two different search systems. And get two sets of results.\n",
    "\n",
    "<img src=\"imgs/judgements.png\" alt=\"Cohere logo\" width=\"450\" height=\"250\"/>\n",
    "\n",
    "### Metric: MAP\n",
    "\n",
    "<img src=\"imgs/map.png\" alt=\"Cohere logo\" width=\"450\" height=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "The leading method the industry turned to remedy this behavior is RAG\n",
    "\n",
    "RAG systems incorporate search capabilities in addition to generation capabilities.\n",
    "\n",
    "## From Search to RAG\n",
    "\n",
    "Let’s now turn our search system into a RAG system. We do that by adding an LLM to the end of the search pipeline. \n",
    "\n",
    "We present the question and the top retrieved documents to the LLM, and ask it to answer the question given the context provided by the search results. \n",
    "\n",
    "<img src=\"imgs/rag.png\" alt=\"Cohere logo\" width=\"450\" height=\"250\"/>\n",
    "\n",
    "This generation step is called **grounded generation** because the retrieved relevant information we provide the LLM establishes a certain context that grounds the LLM in the domain we’re interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LlamaCpp\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"weights/Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model for converting text to numerical representations\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='thenlper/gte-small'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan',\n",
       " 'It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine',\n",
       " 'Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind',\n",
       " 'Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007',\n",
       " 'Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar',\n",
       " 'Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm',\n",
       " 'Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles',\n",
       " 'Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects',\n",
       " 'Interstellar premiered on October 26, 2014, in Los Angeles',\n",
       " 'In the United States, it was first released on film stock, expanding to venues using digital projectors',\n",
       " 'The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014',\n",
       " 'It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight',\n",
       " 'It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics',\n",
       " 'Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time',\n",
       " 'Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local vector database\n",
    "db = FAISS.from_texts(texts, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RAG Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "template = \"\"\"<|user|>\n",
    "Relevant information:\n",
    "{context}\n",
    "\n",
    "Provide a concise answer the following question using the relevant information provided above:\n",
    "{question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# RAG Pipeline\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=db.as_retriever(),\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": prompt\n",
    "    },\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Income generated',\n",
       " 'result': ' The Income generated from the film was over $677 million worldwide, making it the tenth-highest grossing film of 2014.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.invoke('Income generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Techniques\n",
    "\n",
    "\n",
    "### Query rewriting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the RAG system is a chatbot, the preceding simple RAG implementation would likely struggle with the search step **if a question is too verbose**, or to refer to context in previous messages in the conversation.\n",
    "\n",
    "This is why it’s a good idea to use an LLM to **rewrite the query** into one that aids the retrieval step in getting the right information. \n",
    "\n",
    "```text\n",
    "User Question: “We have an essay due tomorrow. We have to write about some animal. I love penguins. I could write about them. But I could also write about dolphins. Are they animals? Maybe. Let’s do dolphins. Where do they live for example?”\n",
    "```\n",
    "\n",
    "Gets:\n",
    "```text\n",
    "Query: “Where do dolphins live”\n",
    "```\n",
    "\n",
    "\n",
    "### Multi-query RAG\n",
    "\n",
    "The next improvement we can introduce is to extend the query rewriting to be able to search multiple queries if more than one is needed to answer a specific question. Take for example:\n",
    "\n",
    "```text\n",
    "User Question: “Compare the financial results of Nvidia in 2020 vs. 2023”\n",
    "```\n",
    "\n",
    "Gets:\n",
    "```text\n",
    "Query 1: “Nvidia 2020 financial results”\n",
    "\n",
    "Query 2: “Nvidia 2023 financial results”\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-hop RAG\n",
    "\n",
    "A more advanced question may require a series of sequential queries.\n",
    "\n",
    "```text\n",
    "User Question: “Who are the largest car manufacturers in 2023? Do they each make EVs or not?”\n",
    "```\n",
    "\n",
    "To answer this, the system must first search for:\n",
    "```text\n",
    "Step 1, Query 1: “largest car manufacturers 2023”\n",
    "```\n",
    "Then receive results:\n",
    "```text\n",
    "Step 2, Query 1: “Toyota Motor Corporation electric vehicles”\n",
    "\n",
    "Step 2, Query 2: “Volkswagen AG electric vehicles”\n",
    "\n",
    "Step 2, Query 3: “Hyundai Motor Company electric vehicles”\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Routing\n",
    "\n",
    "An additional enhancement is to give the model the ability to search **multiple data sources**. \n",
    "\n",
    "\n",
    "Specify to the model if a question is related to topic A, look for in the information system of A, if topic B, look for system B etc... This way we can have separate databases schemas for each topic and ease the search\n",
    "\n",
    "\n",
    "### Agentic RAG\n",
    "\n",
    "The data sources can also now be abstracted into tools. We saw, for example, that we can search Notion, but by the same token, we should be able to post to Notion as well.\n",
    "\n",
    "Cohere’s Command R+ excels at these tasks and is available as an open-weights model as well\n",
    "\n",
    "\n",
    "### RAG Evaluation\n",
    "\n",
    "There are still ongoing developments in how to evaluate RAG models. A good paper to read on this topic is “Evaluating verifiability in generative search engines” (2023), which runs human evaluations on different generative search systems.\n",
    "\n",
    "- Fluency: Whether the generated text is fluent and cohesive.\n",
    "- Perceived utility: Whether the generated answer is helpful and informative.\n",
    "- Citation recall: The proportion of generated statements about the external world that are fully supported by their citations.\n",
    "- Citation precision: The proportion of generated citations that support their associated statements.\n",
    "\n",
    "\n",
    "While human evaluation is always preferred, there are approaches that attempt to automate these evaluations by having a capable **LLM act as a judge (called LLM-as-a-judge)**\n",
    "\n",
    "Ragas is a software library that does exactly this. It also scores some additional useful metrics like:\n",
    "\n",
    "- Faithfulness\n",
    "    - Whether the answer is consistent with the provided context\n",
    "- Answer relevance\n",
    "    - How relevant the answer is to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_hollm",
   "language": "python",
   "name": "kr_hollm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
