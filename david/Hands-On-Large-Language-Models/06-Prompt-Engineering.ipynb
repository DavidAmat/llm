{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183fa06b1bc643aba6daf2f97d94cb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the chicken join the band? Because it had the drumsticks!\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
    "]\n",
    "\n",
    "# Generate the output\n",
    "output = pipe(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, transformers.pipeline first converts our messages into a specific prompt template. We can explore this process by accessing the underlying tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Create a funny joke about chickens.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Apply prompt template\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt template, further illustrated in Figure 6-2, was used during the training of the model. Not only does it provide information about who said what, but it is also used to indicate when the model should stop generating text (see the <|end|> token). This prompt is passed directly to the LLM and processed all at once.\n",
    "\n",
    "![Phi 3 Template](imgs/phi-template.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling Model Output\n",
    "\n",
    "- temperature: The temperature controls the randomness or creativity of the text generated. It defines how likely it is to choose tokens that are less probable\n",
    "- top_p: also known as nucleus sampling, is a sampling technique that controls which subset of tokens (the nucleus) the LLM can consider. It will consider tokens until it reaches their **cumulative probability**\n",
    "- top_k: controls exactly how many tokens the LLM can consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the chicken join the rock band? Because it always knew how to lay down a good beat!\n"
     ]
    }
   ],
   "source": [
    "# Using a high temperature\n",
    "output = pipe(messages, do_sample=True, temperature=1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the rooster get invited to the party? Because he was a great dancer and he knew how to crow-dance all night long!\n"
     ]
    }
   ],
   "source": [
    "# Using a high top_p\n",
    "output = pipe(messages, do_sample=True, top_p=1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Indicators\n",
    "\n",
    "<img src=\"imgs/output_indicators.png\" alt=\"Alt text\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instruction-based prompting\n",
    "\n",
    "Prompting is often used to have the LLM answer a specific question or resolve a certain task\n",
    "\n",
    "<img src=\"imgs/instruction_based_prompting.png\" alt=\"Alt text\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A non-exhaustive list of these techniques includes:\n",
    "\n",
    "- Specificity\n",
    "    - Accurately describe what you want to achieve. Instead of asking the LLM to “Write a description for a product” ask it to “Write a description for a product in less than two sentences and use a formal tone.”\n",
    "- Hallucination\n",
    "    - LLMs may generate incorrect information confidently, which is referred to as hallucination. To reduce its impact, we can ask the LLM to only generate an answer if it knows the answer. If it does not know the answer, it can respond with “I don’t know.”\n",
    "- Order\n",
    "    - Either begin or end your prompt with the instruction. Especially with **long prompts, information in the middle is often forgotten**. \n",
    "    - LLMs tend to focus on information either at the beginning of a prompt (primacy effect) or the end of a prompt (recency effect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Prompt Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In our very first example, our prompt consisted of \n",
    " - instruction\n",
    " - data \n",
    " - output indicators\n",
    "\n",
    " ## Components\n",
    "### Persona\n",
    "Describe what role the LLM should take on. For example, use “You are an expert in astrophysics” if you want to ask a question about astrophysics.\n",
    "### Instruction\n",
    "The task itself. Make sure this is as specific as possible. We do not want to leave much room for interpretation.\n",
    "### Context\n",
    "Additional information describing the context of the problem or task. It answers questions like “What is the reason for the instruction?”\n",
    "### Format\n",
    "The format the LLM should use to output the generated text. Without it, the LLM will come up with a format itself, which is troublesome in automated systems.\n",
    "### Audience\n",
    "The target of the generated text. This also describes the level of the generated output. For education purposes, it is often helpful to use ELI5 (“Explain it like I’m 5”).\n",
    "### Tone\n",
    "The tone of voice the LLM should use in the generated text. If you are writing a formal email to your boss, you might not want to use an informal tone of voice.\n",
    "### Data\n",
    "The main data related to the task itself.\n",
    "\n",
    "### Optional\n",
    "- Stimuli: \"This is very important for my career\"\n",
    "\n",
    "## Modular Prompting\n",
    "<img src=\"imgs/modular_prompting.png\" alt=\"Alt text\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt components\n",
    "persona = \"You are an expert in Large Language models. You excel at breaking down complex papers into digestible summaries.\\n\"\n",
    "instruction = \"Summarize the key findings of the paper provided.\\n\"\n",
    "context = \"Your summary should extract the most crucial points that can help researchers quickly understand the most vital information of the paper.\\n\"\n",
    "data_format = \"Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results.\\n\"\n",
    "audience = \"The summary is designed for busy researchers that quickly need to grasp the newest trends in Large Language Models.\\n\"\n",
    "tone = \"The tone should be professional and clear.\\n\"\n",
    "text = \"MY TEXT TO SUMMARIZE\"\n",
    "data = f\"Text to summarize: {text}\"\n",
    "\n",
    "# The full prompt - remove and add pieces to view its impact on the generated output\n",
    "query = persona + instruction + context + data_format + audience + tone + data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Context Learning: Providing Examples\n",
    "\n",
    "- **Zero-shot prompting** does not leverage examples\n",
    "- **One-shot prompts** use a single example\n",
    "- **Few-shot prompts** use two or more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "A 'Gigamuru' is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:<|end|>\n",
      "<|assistant|>\n",
      "I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.<|end|>\n",
      "<|user|>\n",
      "To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Use a single example of using the made-up word in a sentence\n",
    "one_shot_prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"A 'Gigamuru' is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:\"\n",
    "    }\n",
    "]\n",
    "print(tokenizer.apply_chat_template(one_shot_prompt, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " During the medieval reenactment, the knight skillfully screeged the wooden target, impressing the onlookers with his prowess.\n"
     ]
    }
   ],
   "source": [
    "# Generate the output\n",
    "outputs = pipe(one_shot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Prompting: Breaking up the Problem\n",
    "\n",
    "We take the output of one prompt and use it as input for the next, thereby creating a continuous chain of interactions that solves our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, let us say we want to use an LLM to create a product name, slogan, and sales pitch for us based on a number of product features. Although we can ask the LLM to do this in one go, we can instead break up the problem into pieces.\n",
    "\n",
    "<img src=\"imgs/chain_prompting.png\" alt=\"XXX\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name: ChatSage\n",
      "Slogan: \"Unleashing the power of AI to enhance your conversations.\"\n"
     ]
    }
   ],
   "source": [
    "# Create name and slogan for a product\n",
    "product_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a name and slogan for a chatbot that leverages LLMs.\"}\n",
    "]\n",
    "outputs = pipe(product_prompt)\n",
    "product_description = outputs[0][\"generated_text\"]\n",
    "print(product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Introducing ChatSage, the ultimate AI companion that revolutionizes your conversations. With our cutting-edge technology, we unleash the power of AI to enhance your interactions, making every conversation more engaging and meaningful. Experience the future of communication with ChatSage today!\n"
     ]
    }
   ],
   "source": [
    "# Based on a name and slogan for a product, generate a sales pitch\n",
    "sales_prompt = [\n",
    "    {\"role\": \"user\", \"content\": f\"Generate a very short sales pitch for the following product: '{product_description}'\"}\n",
    "]\n",
    "outputs = pipe(sales_prompt)\n",
    "sales_pitch = outputs[0][\"generated_text\"]\n",
    "print(sales_pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used for a variety of use cases, including:\n",
    "\n",
    "- Response validation\n",
    "    - Ask the LLM to double-check previously generated outputs.\n",
    "- Parallel prompts\n",
    "    - Create multiple prompts in parallel and do a final pass to merge them. For example, ask multiple LLMs to generate multiple recipes in parallel and use the combined result to create a shopping list.\n",
    "- Writing stories\n",
    "    - Leverage the LLM to write books or stories by breaking down the problem into components. For example, by first writing a summary, developing characters, and building the story beats before diving into creating the dialogue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning with Generative Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow for this reasoning behavior, it is a good moment to step back and explore what reasoning entails in human behavior. To simplify, our methods of reasoning can be divided into system 1 and 2 thinking processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System 1:\n",
    "\n",
    "System 1 thinking represents an automatic, intuitive, and near-instantaneous process. \n",
    "It shares similarities with generative models that automatically generate tokens without any self-reflective behavior. \n",
    "\n",
    "### System 2:\n",
    "System 2 thinking is a conscious, slow, and logical process, akin to brainstorming and self-reflection.\n",
    "\n",
    "\n",
    "### Chain Of Thought: Think Before Answering\n",
    "If we could give a generative model the ability to mimic a form of self-reflection, we would essentially be emulating the system 2 way of thinking, which tends to produce more thoughtful responses than system 1 thinking\n",
    "\n",
    "- Chain-of-thought aims to have the generative model “think” first rather than answering the question directly without any reasoning\n",
    "- The reasoning processes are referred to as **thoughts**\n",
    "\n",
    "\n",
    "<img src=\"imgs/chain_of_thought.png\" alt=\"XXX\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The cafeteria started with 25 apples. They used 20 apples to make lunch, so they had 25 - 20 = 5 apples left. After buying 6 more apples, they now have 5 + 6 = 11 apples.\n"
     ]
    }
   ],
   "source": [
    "# Answering without explicit reasoning\n",
    "standard_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"11\"},\n",
    "    {\"role\": \"user\", \"content\": \"The cafeteria had 25 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"}\n",
    "]\n",
    "\n",
    "# Run generative model\n",
    "outputs = pipe(standard_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although chain-of-thought is a great method for enhancing the output of a generative model, it does require one or more examples of reasoning in the prompt, which the user might not have access to.\n",
    "\n",
    "### Zero-shot Chain Of Thought\n",
    "\n",
    "Instead of providing examples, we can simply ask the generative model to provide the reasoning (zero-shot chain-of-thought). By adding particles like:\n",
    "- *Let’s think step-by-step*\n",
    "- *Take a deep breath and think step-by-step*\n",
    "- *Let’s work through this problem step-by-step*\n",
    "- *Think carefully and logically, explaining your answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1: Start with the initial number of apples in the cafeteria, which is 23.\n",
      "\n",
      "Step 2: Subtract the number of apples used to make lunch, which is 20.\n",
      "23 - 20 = 3 apples remaining.\n",
      "\n",
      "Step 3: Add the number of apples bought, which is 6.\n",
      "3 + 6 = 9 apples.\n",
      "\n",
      "So, the cafeteria now has 9 apples.\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot chain-of-thought\n",
    "zeroshot_cot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let's think step-by-step.\"}\n",
    "]\n",
    "\n",
    "# Generate the output\n",
    "outputs = pipe(zeroshot_cot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Consistency\n",
    "\n",
    "As a result, the quality of the output might improve or degrade depending on the random selection of tokens like `temperature` or `top_p`\n",
    "\n",
    "- This method asks the generative model the same prompt multiple times and takes the majority result as the final answer\n",
    "- Each answer can be affected by different temperature and top_p values to increase the diversity of sampling\n",
    "- This process becomes `n` times **slower**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Of-Thought\n",
    "\n",
    "- When faced with a problem that requires multiple reasoning steps, it often helps to break it down into pieces.\n",
    "- The generative model is prompted to explore different solutions to the problem at hand. \n",
    "- It then votes for the best solution and continues to the next step.\n",
    "\n",
    "Pros/Cons:\n",
    "-  A disadvantage of this method is that it requires many calls to the generative models, which slows the application significantly. \n",
    "-  there has been a successful attempt to convert the tree-of-thought framework into a simple prompting technique\n",
    "\n",
    "<img src=\"imgs/ToT.png\" alt=\"XXX\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling the generative model multiple times, we ask the model to mimic that behavior by emulating **a conversation between multiple experts**. \n",
    "\n",
    "```text\n",
    "Imagine three different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realises they're wrong at any point then they leave.\n",
    "The question is...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot tree-of-thought\n",
    "zeroshot_tot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?' Make sure to discuss the results.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Expert 1:\n",
      "Step 1: Start with the initial number of apples, which is 23.\n",
      "\n",
      "Expert 2:\n",
      "Step 1: Subtract the number of apples used for lunch, which is 20. This leaves us with 3 apples.\n",
      "Step 2: Add the number of apples bought, which is 6. This results in a total of 9 apples.\n",
      "\n",
      "Expert 3:\n",
      "Step 1: Begin with the initial number of apples, which is 23.\n",
      "Step 2: Subtract the number of apples used for lunch, which is 20. This leaves us with 3 apples.\n",
      "Step 3: Add the number of apples bought, which is 6. This results in a total of 9 apples.\n",
      "\n",
      "Discussion:\n",
      "All three experts arrived at the same answer, which is 9 apples. This indicates that their calculations were correct. The cafeteria started with 23 apples, used 20 for lunch, and then bought 6 more, resulting in a total of 9 apples.\n"
     ]
    }
   ],
   "source": [
    "# Generate the output\n",
    "outputs = pipe(zeroshot_tot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Verification\n",
    "\n",
    "### Structured output\n",
    "By default, most generative models create free-form text without adhering to specific structures other than those defined by natural language. Some use cases require their output to be structured in certain formats, like JSON.\n",
    "### Valid output\n",
    "Even if we allow the model to generate structured output, it still has the capability to freely generate its content. For instance, when a model is asked to output either one of two choices, it should not come up with a third.\n",
    "### Ethics\n",
    "Some open source generative models have no guardrails and will generate outputs that do not consider safety or ethical considerations. For instance, use cases might require the output to be free of profanity, personally identifiable information (PII), bias, cultural stereotypes, etc.\n",
    "### Accuracy\n",
    "Many use cases require the output to adhere to certain standards or performance. The aim is to double-check whether the generated information is factually accurate, coherent, or free from hallucination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, there are three ways of controlling the output of a generative model:\n",
    "\n",
    "### Examples\n",
    "Provide a number of examples of the expected output.\n",
    "### Grammar\n",
    "Control the token selection process.\n",
    "### Fine-tuning\n",
    "Tune a model on data that contains the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "A simple and straightforward method to fix the output is to provide the generative model with examples of what the output should look like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```json\n",
      "{\n",
      "  \"name\": \"Aria Stormbringer\",\n",
      "  \"class\": \"Warrior\",\n",
      "  \"race\": \"Human\",\n",
      "  \"level\": 10,\n",
      "  \"attributes\": {\n",
      "    \"strength\": 18,\n",
      "    \"dexterity\": 12,\n",
      "    \"constitution\": 16,\n",
      "    \"intelligence\": 8,\n",
      "    \"wisdom\": 10,\n",
      "    \"charisma\": 14\n",
      "  },\n",
      "  \"skills\": {\n",
      "    \"melee\": 15,\n",
      "    \"ranged\": 10,\n",
      "    \"magic\": 5,\n",
      "    \"stealth\": 12,\n",
      "    \"acrobatics\": 10,\n",
      "    \"animal_handling\": 8\n",
      "  },\n",
      "  \"equipment\": {\n",
      "    \"weapon\": \"Two-handed Axe\",\n",
      "    \"armor\": \"Chainmail Hauberk\",\n",
      "    \"shield\": \"Warhammer\",\n",
      "    \"accessories\": [\n",
      "      \"Leather Boots\",\n",
      "      \"Warrior's Bracers\",\n",
      "      \"Warrior's Ring\"\n",
      "    ]\n",
      "  },\n",
      "  \"background\": \"Aria grew up in a small village on the outskirts of a large city. She was always fascinated by the stories of great warriors and adventurers, and dreamed of one day becoming a legendary hero herself. When she was just a child, a group of bandits attacked her village, and Aria was the only one who managed to fend them off. Since then, she has dedicated her life to training and honing her skills as a warrior, and has become one of the most feared fighters in the land.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Zero-shot learning: Providing no examples\n",
    "zeroshot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a character profile for an RPG game in JSON format.\"}\n",
    "]\n",
    "\n",
    "# Generate the output\n",
    "outputs = pipe(zeroshot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "  \"description\": \"A cunning rogue with a mysterious past, skilled in stealth and deception.\",\n",
      "  \"name\": \"Shadowcloak\",\n",
      "  \"armor\": \"Leather Hood\",\n",
      "  \"weapon\": \"Dagger\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# One-shot learning: Providing an example of the output structure\n",
    "one_shot_template = \"\"\"Create a short character profile for an RPG game. Make sure to only use this format:\n",
    "\n",
    "{\n",
    "  \"description\": \"A SHORT DESCRIPTION\",\n",
    "  \"name\": \"THE CHARACTER'S NAME\",\n",
    "  \"armor\": \"ONE PIECE OF ARMOR\",\n",
    "  \"weapon\": \"ONE OR MORE WEAPONS\"\n",
    "}\n",
    "\"\"\"\n",
    "one_shot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": one_shot_template}\n",
    "]\n",
    "\n",
    "# Generate the output\n",
    "outputs = pipe(one_shot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar: Constrained Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot learning has a big disadvantage: we cannot explicitly prevent certain output from being generated\n",
    "\n",
    "- Instead, packages have been rapidly developed to constrain and validate the output of generative models like:\n",
    "    - Guidance\n",
    "    - Guardrails\n",
    "    - LMQL\n",
    "\n",
    "The generative models retrieve the output as new prompts and attempt to validate it based on a number of predefined guardrails.\n",
    "\n",
    "<img src=\"imgs/constrained_sampling.png\" alt=\"XXX\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This validation process can also be used to control the formatting of the output by generating parts of its format ourselves as we already know how it should be structured\n",
    "\n",
    "**We can already perform validation during the token sampling process**\n",
    "\n",
    "When sampling tokens, we can define a number of grammars or rules that the LLM should adhere to when choosing its next token. \n",
    "\n",
    "By constraining the sampling process, we can have the LLM only output what we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "del model, tokenizer, pipe\n",
    "\n",
    "# Flush memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp.llama import Llama\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7642a56ee65845678812e00b2e2eecae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phi-3-mini-4k-instruct-fp16.gguf:   0%|          | 0.00/7.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Phi-3\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "    filename=\"*fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=2048,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Create a warrior for an RPG in JSON format.\"},\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0,\n",
    ")['choices'][0]['message'][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"warrior\": {\n",
      "        \"name\": \"Eldric Stormbringer\",\n",
      "        \"class\": \"Warrior\",\n",
      "        \"level\": 5,\n",
      "        \"attributes\": {\n",
      "            \"strength\": 18,\n",
      "            \"dexterity\": 9,\n",
      "            \"constitution\": 20,\n",
      "            \"intelligence\": 7,\n",
      "            \"wisdom\": 6,\n",
      "            \"charisma\": 10\n",
      "        },\n",
      "        \"skills\": [\n",
      "            {\n",
      "                \"name\": \"Martial Arts\",\n",
      "                \"proficiency\": 95\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Swordsmanship\",\n",
      "                \"proficiency\": 98\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Shield Blocking\",\n",
      "                \"proficiency\": 92\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Heavy Armor Proficiency\",\n",
      "                \"proficiency\": 90\n",
      "            }\n",
      "        ],\n",
      "        \"equipment\": [\n",
      "            {\n",
      "                \"type\": \"Weapon\",\n",
      "                \"name\": \"Iron Longsword\",\n",
      "                \"damage\": 12,\n",
      "                \"durability\": 85\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"Armor\",\n",
      "                \"name\": \"Chainmail Armor\",\n",
      "                \"defense\": 14,\n",
      "                \"protection\": 70\n",
      "            }\n",
      "        ],\n",
      "        \"abilities\": [\n",
      "            {\n",
      "                \"name\": \"Berserker Rage\",\n",
      "                \"description\": \"Increases strength and damage output for a short duration.\",\n",
      "                \"cooldown\": 30,\n",
      "                \"duration\": 120\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Shield Bash\",\n",
      "                \"description\": \"Stuns an enemy with the shield, dealing moderate damage.\",\n",
      "                \"cooldown\": 60,\n",
      "                \"damage\": 8\n",
      "            }\n",
      "        ],\n",
      "        \"inventory\": [\n",
      "            {\n",
      "                \"item_name\": \"Healing Potion\",\n",
      "                \"quantity\": 3\n",
      "            },\n",
      "            {\n",
      "                \"item_name\": \"Iron Ingots\",\n",
      "                \"quantity\": 10,\n",
      "                \"description\": \"Used for crafting or repairing equipment.\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Format as json\n",
    "json_output = json.dumps(json.loads(output), indent=4)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_hollm",
   "language": "python",
   "name": "kr_hollm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
