{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from llama_cpp import Llama\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model I/O\n",
    "    - Loading and working with LLMs\n",
    "- Memory\n",
    "    - Helping LLMs to remember\n",
    "- Agents\n",
    "    - Combining complex behavior with external tools\n",
    "- Chains\n",
    "    - Connecting methods and modules\n",
    "\n",
    "Newer frameworks of note are:\n",
    "- DSPy\n",
    "- Haystack\n",
    "\n",
    "<img src=\"imgs/langchain.png\" alt=\"XXX\" width=\"400\" height=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model I/O: Loading Quantized Models with LangChain\n",
    "\n",
    "A GGUF model represents a compressed version of its original counterpart through a method called quantization\n",
    "\n",
    "Quantization reduces the number of bits required to represent the parameters of an LLM while attempting to maintain most of the original information. This comes with some loss in precision but often makes up for it as the model is much faster to run, requires less VRAM, and is often almost as accurate as the original.\n",
    "\n",
    "`As a rule of thumb, look for at least 4-bit quantized models. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"weights/Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in previous chapters, Phi-3 requires a specific prompt template. \n",
    "\n",
    " Instead of copy-pasting this template each time we use Phi-3 in LangChain, we can use one of LangChain’s core functionalities, namely “chains.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains: Extending the Capabilities of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a chain can take many forms, each with a different complexity, it generally connects an LLM with some additional tool, prompt, or feature.\n",
    "\n",
    "<img src=\"imgs/chains.png\" alt=\"XXX\" width=\"400\" height=\"100\">\n",
    "\n",
    "Here, a single chain connects to a single modular component like a prompt template or external memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "\n",
    "With LangChain, we will use chains to create and use a default prompt template. We'll chain the prompt template together with the LLM to get the output we are looking for.\n",
    "\n",
    "The template for Phi-3 is comprised of four main components:\n",
    "\n",
    "```text\n",
    "- <s> to indicate when the prompt starts\n",
    "- <|user|> to indicate the start of the user’s prompt\n",
    "- <|assistant|> to indicate the start of the model’s output\n",
    "- <|end|> to indicate the end of either the prompt or the model’s output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template with the \"input_prompt\" variable\n",
    "template = \"\"\"<|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input_prompt'], template='<|user|>\\n{input_prompt}<|end|>\\n<|assistant|>')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our first chain, we can use both the prompt that we created and the LLM and chain them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello Maarten! The answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the chain\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders for varying inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chain that creates our business' name\n",
    "template = \"\"\"<|user|>\n",
    "Create a funny name for a business that sells {product}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "name_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"product\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_chain = name_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Zoogle: The Littlest Legos for Zoo Enthusiasts\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the chain\n",
    "product_chain.invoke(\n",
    "    {\n",
    "        \"product\": \"Small lego pieces of zoo animals\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Chains\n",
    "\n",
    "We could break this complex prompt into smaller subtasks that can be run sequentially. This would require multiple calls to the LLM but with smaller prompts and intermediate outputs\n",
    "\n",
    "**Example**\n",
    "Let’s illustrate with an example. Assume that we want to generate a story that has three components:\n",
    "\n",
    "- A title\n",
    "- A description of the main character\n",
    "- A summary of the story\n",
    "\n",
    "<img src=\"imgs/story.png\" alt=\"XXX\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This first link is the only component that requires some input from the user. We define the template and use the \"summary\" variable as the input variable and \"title\" as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Title From a Summary of the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "class M(BaseModel):\n",
    "    summary: str\n",
    "    title: str\n",
    "parser = JsonOutputParser(pydantic_object=M)\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"weights/Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Create a chain for the title of our story\n",
    "template = \"\"\"\n",
    "<|user|>\n",
    "Create a title for a story with the following summary: {summary}. {format_instructions}\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "title_prompt = PromptTemplate.from_template(\n",
    "    template=template,\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "title_chain = title_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "summary = \"a girl that lost her mother\"\n",
    "result = title_chain.invoke({\"summary\": summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'A poignant tale of loss and resilience as a young girl navigates life without her mother.',\n",
       " 'title': \"Whispers from the Past: A Girl's Journey Through Grief\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Description of the character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Desc(BaseModel):\n",
    "    summary: str\n",
    "    title: str\n",
    "    character_description: str\n",
    "parser = JsonOutputParser(pydantic_object=Desc)\n",
    "\n",
    "# Create a chain for the character description using the summary and title\n",
    "template = \"\"\"<|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}. Put the description of the character in the \"character_description\" property of the output json and repeat the summary and title in the output format. {format_instructions}<|end|> \n",
    "<|assistant|>\"\"\"\n",
    "description_prompt = PromptTemplate.from_template(\n",
    "    template=template,\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_chain = title_prompt | llm | parser | description_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "summary = \"a girl that lost her mother\"\n",
    "# result = description_chain.invoke({\"summary\": summary})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Llama model from Llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "    model_path=\"weights/Llama-3.2-3B-Instruct-Q4_K_M.gguf\",\n",
    "    n_ctx=2048,  # Drastically reduce context window\n",
    "    n_batch=128,  # Smaller batch size\n",
    "    n_gpu_layers=-1,  # GPU layers\n",
    "    n_threads=4,  # Fewer threads\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the main benefits of artificial intelligence include:\n",
      "\n",
      "*   **Increased Efficiency**: AI can automate repetitive tasks, freeing up human time and resources for more complex and creative tasks.\n",
      "*   **Improved Accuracy**: AI can process large amounts of data quickly and accurately, reducing the risk of human error.\n",
      "*   **Enhanced Decision-Making**: AI can analyze vast amounts of data to identify patterns and make predictions, helping organizations make better decisions.\n",
      "*   **Personalization**: AI can help personalize experiences for customers, making interactions more engaging and relevant.\n",
      "*   **Innovation**: AI can enable new products and services, and improve existing ones, driving business growth and competitiveness.\n",
      "\n",
      "These are just a few examples of the many benefits of artificial intelligence. Do you have any specific areas you'd like to know more about? I'm here to help!\n"
     ]
    }
   ],
   "source": [
    "# Use a more structured prompt\n",
    "prompt = f\"<|begin_of_text|>Human: What are the main benefits of artificial intelligence?\\n\\nAssistant:\"\n",
    "\n",
    "output = model(\n",
    "    prompt, \n",
    "    max_tokens=500,  # Limit response length\n",
    "    stop=[\"Human:\", \"<|end_of_text|>\"],  # Appropriate stop tokens\n",
    "    echo=False  # Don't repeat the prompt\n",
    ")\n",
    "\n",
    "# Extract the actual text response\n",
    "full_response = output['choices'][0]['text'].strip()\n",
    "print(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Multiple Chains with LLamaCpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. Respond ONLY in valid JSON format matching this structure:\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "joke_output = chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline=\"Because even they can't make up anything!\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n<|assistant|> Why don't chickens play poker in the coop? Too many chicken cards!\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "#   Phi-3\n",
    "##################################\n",
    "# Continue the sequence\n",
    "joke_query = \"Tell me a joke about chickens.\"\n",
    "response = llm(joke_query, max_tokens=200, echo=False)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-a47bbf41-15d9-45c1-af53-c00798e84b2a',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1733823224,\n",
       " 'model': 'weights/Llama-3.2-3B-Instruct-Q4_K_M.gguf',\n",
       " 'choices': [{'text': ' here\\n\\nA chicken walks into a library and says to the librarian \"do you have any books on Pavlov\\'s dogs and Schrödinger\\'s cat?\" The librarian replies, \"it rings a bell, but I\\'m not sure if it\\'s here or not.\"',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 8, 'completion_tokens': 55, 'total_tokens': 63}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "#   Llama 3.2\n",
    "##################################\n",
    "# Continue the sequence\n",
    "joke_query = \"Tell me a joke about chickens.\"\n",
    "response = model(joke_query, max_tokens=200, echo=False)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic Output Parser With Title Generation Chained with Character Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ #\n",
    "#   Chain 1: Title Generation    #\n",
    "# ------------------------------ #\n",
    "\n",
    "# Define the schema for the title generation output\n",
    "class TitleOutput(BaseModel):\n",
    "    summary: str = Field(description=\"The summary of the story\")\n",
    "    title: str = Field(description=\"A concise, creative title for the story\")\n",
    "\n",
    "# Initialize the parser for title output\n",
    "title_parser = PydanticOutputParser(pydantic_object=TitleOutput)\n",
    "\n",
    "\n",
    "# Prompt for title generation\n",
    "template_title = \"\"\"\n",
    "Create a title for a story with the following summary: {summary}. Respond ONLY in valid JSON format matching this structure:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "title_prompt = PromptTemplate(\n",
    "    template=template_title,\n",
    "    input_variables=[\"summary\"],\n",
    "    partial_variables={\"format_instructions\": title_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Create the chain for title generation\n",
    "title_chain = title_prompt | llm | title_parser\n",
    "\n",
    "# Example usage\n",
    "# summary = \"A girl that lost her mother.\"\n",
    "# result = title_chain.run({\"summary\": summary})\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary=\"A Girl's Journey Through Loss and Discovery: The Tale of a Motherless Child\" title='Echoes of Love: A Story Beyond Goodbye'\n"
     ]
    }
   ],
   "source": [
    "summary = \"A girl that lost her mother.\"\n",
    "result = title_chain.invoke({\"summary\": summary})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------- #\n",
    "#   Chain 2: Character Description   #\n",
    "# ---------------------------------- #\n",
    "\n",
    "# Define the schema for character description output\n",
    "class CharacterDescriptionOutput(BaseModel):\n",
    "    summary: str = Field(description=\"The summary of the story\")\n",
    "    title: str = Field(description=\"The title of the story\")\n",
    "    character_description: str = Field(description=\"A detailed description of the main character\")\n",
    "\n",
    "# Initialize the parser for character description\n",
    "description_parser = PydanticOutputParser(pydantic_object=CharacterDescriptionOutput)\n",
    "\n",
    "# Prompt for character description\n",
    "template_descript = \"\"\"\n",
    "With the input data:\n",
    "- Summary: {summary}\n",
    "- Title: {title}\n",
    "Describe the main character (character_description) of a story about the given summary with the given title.\n",
    "Respond ONLY in valid JSON format matching this structure and DO NOT add any additional text before or after the JSON:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "description_prompt = PromptTemplate(\n",
    "    template=template_descript,\n",
    "    input_variables=[\"summary\", \"title\"],\n",
    "    partial_variables={\"format_instructions\": description_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Chain for character description\n",
    "description_chain = description_prompt | llm | description_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for the full pipeline\n",
    "summary = \"A girl that lost her mother\"\n",
    "# Generate the title\n",
    "title_result = title_chain.invoke({\"summary\": summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Echoes of a Mother Lost'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_result.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A girl that lost her mother'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the character description using the title\n",
    "description_result = description_chain.invoke({\"summary\": summary, \"title\": title_result.title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'A girl that lost her mother',\n",
       " 'title': 'Echoes of a Mother Lost',\n",
       " 'character_description': \"The main character is an emotive young girl who has experienced the unimaginable loss of her mother at a tender age. She embodies resilience and strength, despite grappling with deep-seated grief and longing for maternal love that once guided her life. This poignant journey explores how she navigates through the labyrinth of memories and emotions tied to her mother's absence.\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_result.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La ciudad de Chicago, Illinois, está en los Estados Unidos.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ #\n",
    "#   Chain 1: Title Generation    #\n",
    "# ------------------------------ #\n",
    "\n",
    "# Prompt for title generation\n",
    "template_title = \"\"\"\n",
    "Create a title for a story with the following summary: {summary}. Respond ONLY the title.\n",
    "\"\"\"\n",
    "title_prompt = PromptTemplate(\n",
    "    template=template_title,\n",
    "    input_variables=[\"summary\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ #\n",
    "#   Chain 2: Character Description   #\n",
    "# ------------------------------ #\n",
    "\n",
    "# Define the schema for character description output\n",
    "class CharacterDescriptionOutput(BaseModel):\n",
    "    summary: str = Field(description=\"The summary of the story\")\n",
    "    title: str = Field(description=\"The title of the story\")\n",
    "    character_description: str = Field(description=\"A detailed description of the main character\")\n",
    "\n",
    "# Initialize the parser for character description\n",
    "description_parser = PydanticOutputParser(pydantic_object=CharacterDescriptionOutput)\n",
    "# description_parser = JsonOutputParser()\n",
    "\n",
    "# Prompt for character description\n",
    "template_descript = \"\"\"\n",
    "With the input data:\n",
    "- Summary: {summary}\n",
    "- Title: {title}\n",
    "Describe the main character of a story about the given summary and title.\n",
    "Respond ONLY in valid JSON of the form (replace placeholders){format_instructions}\n",
    "\"\"\"\n",
    "description_prompt = PromptTemplate(\n",
    "    template=template_descript,\n",
    "    input_variables=[\"summary\", \"title\"],  # Ensure \"character_description\" is not here\n",
    "    partial_variables={\"format_instructions\": description_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'A girl that lost her mother.',\n",
       " 'title': 'Whispers of Loss',\n",
       " 'character_description': 'The main character is a young girl named Emily who is dealing with the profound loss of her mother. She is quiet and introspective, finding solace in nature and the gentle whispers of the wind. Emily often retreats into her own world, trying to navigate the complex emotions that come with grief. Despite her sorrow, she possesses a quiet strength and resilience that helps her cope with the pain of her loss.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------ #\n",
    "#   Combine Chains Using \n",
    "# ------------------------------ #\n",
    "\n",
    "# Chain for title generation\n",
    "chain_title = title_prompt | model | StrOutputParser()\n",
    "chain_description = (\n",
    "    {\"summary\": itemgetter(\"summary\"), \"title\": chain_title}\n",
    "    | description_prompt\n",
    "    | model\n",
    "    | description_parser\n",
    ")\n",
    "\n",
    "# ------------------------------ #\n",
    "#   Example Usage of Combined Pipeline   #\n",
    "# ------------------------------ #\n",
    "summary = \"A girl that lost her mother.\"\n",
    "result = chain_description.invoke({\"summary\": summary})\n",
    "result.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterDescriptionOutput(summary='A girl that lost her mother.', title='Whispers of the Past', character_description=\"The main character is a young girl named Emily who is grieving the loss of her mother. She is quiet and introspective, often retreating into her own world to cope with her emotions. Despite her sadness, Emily is incredibly resilient and carries a deep sense of strength within her. She finds solace in listening to the whispers of the past, believing that her mother's spirit is guiding her through the difficult times.\")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------ #\n",
    "#   Debug individual chains\n",
    "# ------------------------------ #\n",
    "chain_description = (\n",
    "    description_prompt\n",
    "    | model\n",
    "    | description_parser\n",
    ")\n",
    "print(chain_description.invoke({\"summary\": summary, \"title\": \"Whispers of the Past\"}))\n",
    "\n",
    "\n",
    "chain_title = (\n",
    "    {\"summary\": itemgetter(\"summary\")}\n",
    "    | title_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain_title.invoke({\"summary\": summary}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_hollm",
   "language": "python",
   "name": "kr_hollm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
