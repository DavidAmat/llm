{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import TrainConfig\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/r_unfair/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import LexGlueDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = LexGlueDataModule(\n",
    "    pretrained_model=train_config.pretrained_model,\n",
    "    max_length=train_config.max_length,\n",
    "    batch_size=train_config.batch_size,\n",
    "    num_workers=train_config.num_workers,\n",
    "    debug_mode_sample=train_config.debug_mode_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch\n",
    "datamodule.setup(\"fit\")\n",
    "\n",
    "# Create a DataLoader with no parallelization\n",
    "data_loader = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/r_unfair/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.\n",
      "In addition, using fork() with Python in general is a recipe for mysterious\n",
      "deadlocks and crashes.\n",
      "\n",
      "The most likely reason you are seeing this error is because you are using the\n",
      "multiprocessing module on Linux, which uses fork() by default. This will be\n",
      "fixed in Python 3.14. Until then, you want to use the \"spawn\" context instead.\n",
      "\n",
      "See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.\n",
      "\n",
      "If you really know what your doing, you can silence this warning with the warning module\n",
      "or by setting POLARS_ALLOW_FORKING_THREAD=1.\n",
      "\n",
      "  self.pid = os.fork()\n",
      "/home/david/anaconda3/envs/r_unfair/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.\n",
      "In addition, using fork() with Python in general is a recipe for mysterious\n",
      "deadlocks and crashes.\n",
      "\n",
      "The most likely reason you are seeing this error is because you are using the\n",
      "multiprocessing module on Linux, which uses fork() by default. This will be\n",
      "fixed in Python 3.14. Until then, you want to use the \"spawn\" context instead.\n",
      "\n",
      "See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.\n",
      "\n",
      "If you really know what your doing, you can silence this warning with the warning module\n",
      "or by setting POLARS_ALLOW_FORKING_THREAD=1.\n",
      "\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Input IDs: torch.Size([256, 128])\n",
      "Attention Mask: torch.Size([256, 128])\n",
      "Labels: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Fetch a single batch\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "    input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"label\"]\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(f\"Input IDs: {input_ids.shape}\")\n",
    "    print(f\"Attention Mask: {attention_mask.shape}\")\n",
    "    print(f\"Labels: {labels.shape}\")\n",
    "    break  # Only fetch the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading PEFT params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "pretrained_model_name = train_config.pretrained_model\n",
    "lora_weights_path = \"model-checkpoints/lora_adapters.pt\"\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect LORA Adapters Saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_720231/3665936600.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora_weights = torch.load(lora_weights_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Load the saved LoRA adapter weights\n",
    "lora_weights = torch.load(lora_weights_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys and shapes of tensors in the LoRA adapters:\n"
     ]
    }
   ],
   "source": [
    "# Inspect the keys and shapes of the tensors\n",
    "print(\"Keys and shapes of tensors in the LoRA adapters:\")\n",
    "for key, tensor in lora_weights.items():\n",
    "    # print(f\"Name: {key}, Shape: {tensor.shape}, Data Type: {tensor.dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=pretrained_model_name,\n",
    "    num_labels=num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add LORA adapter layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=True,  # Set to True for inference\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model = get_peft_model(base_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.roberta.embeddings.word_embeddings.weight', 'base_model.model.roberta.embeddings.position_embeddings.weight', 'base_model.model.roberta.embeddings.token_type_embeddings.weight', 'base_model.model.roberta.embeddings.LayerNorm.weight', 'base_model.model.roberta.embeddings.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.0.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.0.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.0.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.0.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.0.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.0.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.0.output.dense.weight', 'base_model.model.roberta.encoder.layer.0.output.dense.bias', 'base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.1.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.1.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.1.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.1.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.1.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.1.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.1.output.dense.weight', 'base_model.model.roberta.encoder.layer.1.output.dense.bias', 'base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.2.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.2.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.2.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.2.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.2.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.2.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.2.output.dense.weight', 'base_model.model.roberta.encoder.layer.2.output.dense.bias', 'base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.3.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.3.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.3.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.3.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.3.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.3.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.3.output.dense.weight', 'base_model.model.roberta.encoder.layer.3.output.dense.bias', 'base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.4.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.4.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.4.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.4.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.4.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.4.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.4.output.dense.weight', 'base_model.model.roberta.encoder.layer.4.output.dense.bias', 'base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.5.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.5.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.5.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.5.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.5.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.5.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.5.output.dense.weight', 'base_model.model.roberta.encoder.layer.5.output.dense.bias', 'base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.6.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.6.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.6.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.6.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.6.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.6.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.6.output.dense.weight', 'base_model.model.roberta.encoder.layer.6.output.dense.bias', 'base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.7.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.7.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.7.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.7.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.7.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.7.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.7.output.dense.weight', 'base_model.model.roberta.encoder.layer.7.output.dense.bias', 'base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.8.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.8.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.8.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.8.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.8.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.8.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.8.output.dense.weight', 'base_model.model.roberta.encoder.layer.8.output.dense.bias', 'base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.9.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.9.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.9.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.9.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.9.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.9.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.9.output.dense.weight', 'base_model.model.roberta.encoder.layer.9.output.dense.bias', 'base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.10.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.10.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.10.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.10.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.10.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.10.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.10.output.dense.weight', 'base_model.model.roberta.encoder.layer.10.output.dense.bias', 'base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight', 'base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.bias', 'base_model.model.roberta.encoder.layer.11.attention.self.key.weight', 'base_model.model.roberta.encoder.layer.11.attention.self.key.bias', 'base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight', 'base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.bias', 'base_model.model.roberta.encoder.layer.11.attention.output.dense.weight', 'base_model.model.roberta.encoder.layer.11.attention.output.dense.bias', 'base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.model.roberta.encoder.layer.11.intermediate.dense.weight', 'base_model.model.roberta.encoder.layer.11.intermediate.dense.bias', 'base_model.model.roberta.encoder.layer.11.output.dense.weight', 'base_model.model.roberta.encoder.layer.11.output.dense.bias', 'base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight', 'base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias', 'base_model.model.classifier.original_module.dense.weight', 'base_model.model.classifier.original_module.dense.bias', 'base_model.model.classifier.original_module.out_proj.weight', 'base_model.model.classifier.original_module.out_proj.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overwrite the adapter weights\n",
    "model.load_state_dict(lora_weights, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.functional import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    # Pass the batch through the model\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "    \n",
    "    # Extract logits\n",
    "    logits = outputs.logits  # Shape: [batch_size, num_classes]\n",
    "    \n",
    "    # Convert logits to predicted class labels\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Shape: [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predicted labels with true labels\n",
    "true_labels = labels  # Shape: [batch_size]\n",
    "\n",
    "num_classes = train_config.num_classes  # Assuming the model config holds the number of labels\n",
    "\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_value = accuracy(predictions, true_labels, task=\"multiclass\", num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy%: 99.61\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "#print(f\"Predicted Labels: {predictions}\")\n",
    "#print(f\"True Labels: {true_labels}\")\n",
    "print(f\"Accuracy%: {accuracy_value.item() * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_unfair",
   "language": "python",
   "name": "kr_unfair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
