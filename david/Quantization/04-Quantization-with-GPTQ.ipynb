{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-bit Quantization with GPTQ\n",
    "\n",
    "Source: https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base model and output directory\n",
    "model_id = \"gpt2\"\n",
    "out_dir = model_id + \"-GPTQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTQ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptqmodel import GPTQModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 8 files: 100%|██████████| 8/8 [00:39<00:00,  4.88s/it]\n",
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but backend is not BACKEND.MARLIN. We recommend using `backend=BACKEND.MARLIN` to use the optimized Marlin kernels for inference. Example: `model = GPTQModel.from_quantized(..., backend=BACKEND.MARLIN)`.\n",
      "INFO - Auto pick kernel based on compatibility: <class 'gptqmodel.nn_modules.qlinear.marlin.MarlinQuantLinear'>\n",
      "INFO - Compatibility: converting `checkpoint_format` from `gptq` to `gptq_v2`.\n"
     ]
    }
   ],
   "source": [
    "model = GPTQModel.load(\"ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Llama, created by Meta. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Create a simple linked list in Python\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   2304,   4448,    220,   2366,     20,    271,   2675,    527,\n",
       "            445,  81101,     11,   3549,    555,  16197,     13,   1472,    527,\n",
       "            264,  11190,  18328,     13, 128009, 128006,    882, 128007,    271,\n",
       "           4110,    264,   4382,  10815,   1160,    304,  13325, 128009, 128006,\n",
       "          78191, 128007,    271]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 57])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids=input_tensor.to(model.device), max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the problem: Optimal Brain Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Brain Quantization\n",
    "\n",
    "Let’s start by introducing the problem we’re trying to solve. For every layer $\\ell$ in the network, we want to find a quantized version $\\hat{\\mathbf{W}}_{\\ell}$ of the original weights $\\mathbf{W}_{\\ell}$. This is called the *layer-wise compression problem*. More specifically, to minimize performance degradation, we want the outputs $(\\hat{\\mathbf{W}}_{\\ell}\\,\\mathbf{X}_{\\ell})$ of these new weights to be as close as possible to the original ones $(\\mathbf{W}_{\\ell}\\,\\mathbf{X}_{\\ell})$. In other words, we want to find:\n",
    "\n",
    "$$\n",
    "\\arg \\min_{\\hat{\\mathbf{W}}_{\\ell}} \\bigl\\|\\mathbf{W}_{\\ell}\\,\\mathbf{X}_{\\ell} \\;-\\; \\hat{\\mathbf{W}}_{\\ell}\\,\\mathbf{X}_{\\ell}\\bigr\\|_{2}^{2}.\n",
    "$$\n",
    "\n",
    "Different approaches have been proposed to solve this problem, but we’re interested in the **Optimal Brain Quantizer (OBQ)** framework here.\n",
    "\n",
    "This method is inspired by a pruning technique to carefully remove weights from a fully trained dense neural network (Optimal Brain Surgeon). It uses an approximation technique and provides explicit formulas for the best single weight $w_{q}$ to remove and optimal update $\\delta_{F}$ to adjust the set of remaining non-quantized weights $F$ to make up for the removal:\n",
    "\n",
    "$$\n",
    "w_{q} \\;=\\; \\arg\\min_{w_{q}} \\;\\frac{\\bigl(\\mathrm{quant}(w_{q}) - w_{q}\\bigr)^{2}}{\\bigl[H_{F}^{-1}\\bigr]_{qq}}, \n",
    "\\quad\n",
    "\\delta_{F} \\;=\\; -\\, \\frac{w_{q} \\;-\\; \\mathrm{quant}(w_{q})}{\\bigl[H_{F}^{-1}\\bigr]_{qq}} \\;\\cdot\\; (H_{F}^{-1})_{:q}.\n",
    "$$\n",
    "\n",
    "where $\\mathrm{quant}(w)$ is the weight rounding given by the quantization and $H_{F}$ is the Hessian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Brain Surgeon\n",
    "\n",
    "Optimal Brain Surgeon (OBS) was introduced in the early 1990s (Hassibi & Stork, 1993) as a technique to prune neural networks by removing weights one at a time while minimizing the increase in the training loss. \n",
    "\n",
    "- **Pruning (or quantizing) a weight** is viewed as constraining it to a new (often smaller) value—in classical pruning, that new value might be zero; in quantization, it might be a discrete number from a small set.\n",
    "\n",
    "- Consider that the neural network is highly over-parameterized, and changing a single weight can often be **compensated by small adjustments in the others**\n",
    "\n",
    "**OBS provides a systematic way to find and apply those compensations**, using second-order approximations of the loss function.\n",
    "\n",
    "After training a normal network we reach $\\nabla_{\\mathbf{w}} L(\\mathbf{w}^*) \\;=\\; \\mathbf{0}.$\n",
    "\n",
    "Now imagine we make a small change (or “perturbation”). This perturbation can be approximated via a second-order Taylor expansion:\n",
    "\n",
    "$$L(\\mathbf{w}^* + \\Delta \\mathbf{w})\n",
    "\\;\\approx\\; L(\\mathbf{w}^*) \\;+\\;\n",
    "\\underbrace{\\nabla_{\\mathbf{w}} L(\\mathbf{w}^*)}_{=\\mathbf{0}}^\\top \\,\\Delta \\mathbf{w}\n",
    "\\;+\\; \\tfrac{1}{2}\\,\\Delta \\mathbf{w}^\\top \\underbrace{\\nabla_{\\mathbf{w}}^2 L(\\mathbf{w}^*)}_{H} \\,\\Delta \\mathbf{w},$$\n",
    "\n",
    "where $H = \\nabla_{\\mathbf{w}}^2 L(\\mathbf{w}^*)$ is the Hessian matrix evaluated at $w^*$\n",
    "\n",
    "Since we are in a local minimum, it is given that $\\nabla_{\\mathbf{w}} L(\\mathbf{w}^*)=\\mathbf{0}$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\Delta L \\;=\\;\n",
    "L(\\mathbf{w}^* + \\Delta \\mathbf{w}) - L(\\mathbf{w}^*)\n",
    "\\;\\approx\\; \\tfrac{1}{2} \\,\\Delta \\mathbf{w}^\\top H \\,\\Delta \\mathbf{w}.$$\n",
    "\n",
    "More reasoning of the proof is provided but unnecessary to get why we need the Hessian of the loss for the set of parameters $F$ that will compensate for the pruned or quantized weight\n",
    "\n",
    "### IMPORTANT NOTE:\n",
    "When you see an expression like\n",
    "\n",
    "$$\n",
    "\\mathrm{quant}(w_{q}) \\;-\\; w_{q},\n",
    "$$\n",
    "\n",
    "it is important to note that $\\mathrm{quant}(\\cdot)$ usually refers **not** just to mapping $w_q$ to an integer, but rather **the round-trip quantize–dequantize operation**. In other words:\n",
    "\n",
    "1. **Quantize** a floating-point value $w_q$ into an integer (e.g. in $[0, 255]$ for 8-bit).\n",
    "2. **Dequantize** that integer back into a floating-point approximation (which lives in the same domain as the original $w_q$).\n",
    "\n",
    "Thus, $\\mathrm{quant}(w_{q})$ ends up being a floating-point number **close to** $w_q$, but restricted to the discrete set of representable values allowed by your quantization scheme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming back to our formulas for OBQ\n",
    "\n",
    "$$\n",
    "w_{q} \\;=\\; \\arg\\min_{w_{q}} \\;\\frac{\\bigl(\\mathrm{quant}(w_{q}) - w_{q}\\bigr)^{2}}{\\bigl[H_{F}^{-1}\\bigr]_{qq}}, $$\n",
    "\n",
    "$$\n",
    "\\delta_{F} \\;=\\; -\\, \\frac{w_{q} \\;-\\; \\mathrm{quant}(w_{q})}{\\bigl[H_{F}^{-1}\\bigr]_{qq}} \\;\\cdot\\; (H_{F}^{-1})_{:q}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OBQ, we can quantize the easiest weight first and then adjust all remaining non-quantized weights to compensate for this precision loss. Then we pick the next weight to quantize, and so on.\n",
    "\n",
    "**Problem: Outliers**: A potential issue with this approach is when there are outlier weights, which can result in high quantization error. Usually, these outliers would be quantized last, when there are few non-quantized weights left that could be adjusted to compensate for the large error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with the high computationally intensive task this supposes\n",
    "\n",
    "This process could be computationally heavy, especially for LLMs. To deal with this, the OBQ method uses a trick that avoids redoing the entire computation each time a weight is simplified.\n",
    "\n",
    "After quantizing a weight, it adjusts the matrix used in calculations (the Hessian) by removing the row and column associated with that weight (using Gaussian elimination).\n",
    "\n",
    "The scaling is **cubic**. This cubic growth makes it difficult to use OBQ on very large models with billions of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The GPTQ Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scales the OBQ method\n",
    "\n",
    "The general procedure in GPTQ:\n",
    "\n",
    "- Partition $W$ into blocks. For example, in GPTQ-for-LLaMA, **each transformer layer might be broken into smaller sub-matrices**, or you use one block per layer.\n",
    "- **Quantize a block using an iterative second-order approach**, sometimes referred to as “Error Compensation.”\n",
    "- **Update the residual or remainder of the block** (and possibly other blocks) to compensate for the error.\n",
    "- Repeat until all blocks are quantized.\n",
    "\n",
    "### Mathematical derivation\n",
    "\n",
    "Start with a **block of weights**:\n",
    "\n",
    "$$ \\mathbf{W}_{b} \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "The idea is to quantize (some or all of) these parameters from high precision (e.g., float16) to a discrete set $\\mathbf{Q}$. **GPTQ proceeds iteratively, often focusing on one “row” or “column” (or small set of columns) at a time within the block.**\n",
    "\n",
    "#### Second-Order Approximation\n",
    "\n",
    "We start with a second-order Taylor expansion of the loss function around the current trained weights $w^*$:\n",
    "\n",
    "$$\\Delta \\mathcal{L}(\\Delta \\mathbf{w})\n",
    "\\;\\approx\\;\n",
    "\\frac{1}{2}\n",
    "\\,\\Delta \\mathbf{w}^\\top\n",
    "H\n",
    "\\,\\Delta \\mathbf{w},$$\n",
    "\n",
    "**Goal**: If we decide to quantize a subset of the parameters $\\mathbf{w}_Q\\subset \\mathbf{w}$ to discrete values, we can allow a compensatory update $\\Delta \\mathbf{w}_F$ in the \"free\" subset of parameters $\\mathbf{w}_F$. The main **goal** is to minimize the second-order loss increase.\n",
    "\n",
    "$$\\Delta \\mathbf{w}_Q\n",
    "\\;=\\; \\mathrm{quant}(\\mathbf{w}_Q^*) \\;-\\; \\mathbf{w}_Q^*,\n",
    "\\quad\\quad\n",
    "\\mathbf{w}_Q^* \\,\\text{are the original values}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-by-One or Small-Group Updates\n",
    "\n",
    "A common practical approach is:\n",
    "\n",
    "- Pick a single row (or small group) of $\\mathbf{W}_b$\n",
    "- Attempt to quantize it from float16 to 4-bit.\n",
    "- Solve for how to best update the unquantized portion of that row (or block) to compensate.\n",
    "- Move on to the next row/group.\n",
    "\n",
    "\n",
    "### Minimization\n",
    "\n",
    "To minimize $\\Delta \\mathcal{L}$ we solve for $\\Delta \\mathbf{w}_F$. In a small group context, this can be done with a local Hessian or a block of the Hessian, sometimes denoted $H_b$. Then we do:\n",
    "\n",
    "$$\\Delta \\mathbf{w}_F\n",
    "\\;=\\;\n",
    "\\arg \\min_{\\Delta \\mathbf{w}_F}\n",
    "\\;\n",
    "\\tfrac{1}{2}\n",
    "\\begin{pmatrix}\n",
    "\\Delta \\mathbf{w}_Q \\\\\n",
    "\\Delta \\mathbf{w}_F\n",
    "\\end{pmatrix}^\\top\n",
    "H_b\n",
    "\\begin{pmatrix}\n",
    "\\Delta \\mathbf{w}_Q \\\\\n",
    "\\Delta \\mathbf{w}_F\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "\n",
    "Because $\\Delta \\mathbf{w}_Q$ is fixed, we can differentiate and set to zero with only respect to $\\Delta \\mathbf{w}_F$. This yields a closed-form update if $H_b$ is well-defined (invertible). In practice, GPTQ uses approximations or partial factorization of $H_b$.\n",
    "\n",
    "### Final Summary\n",
    "\n",
    "\n",
    "1. **Initialize**: $\\mathbf{W}_b \\leftarrow \\mathbf{W}_b^*$ (the trained block weights).  \n",
    "2. **Compute Hessian Approx**: $\\tilde{H}_b \\approx \\nabla^2_{\\mathbf{W}_b} \\mathcal{L}$. (In practice, may only store or invert partial info.)  \n",
    "3. **Partition** $\\mathbf{W}_b$ into rows or columns that you will quantize in small sets.  \n",
    "4. **For each row/column** $r$ in block $b$:  \n",
    "   - Compute $\\Delta w_j = \\mathrm{quant}(w_j^*) - w_j^*$ for $j \\in r$.  \n",
    "   - Solve for $\\Delta \\mathbf{w}_F$ in the “free” weights to minimize $\\Delta \\mathcal{L} \\approx \\tfrac{1}{2}\\,\\Delta \\mathbf{w}^\\top \\tilde{H}_b \\,\\Delta \\mathbf{w}$.  \n",
    "   - Update $\\mathbf{W}_b \\leftarrow \\mathbf{W}_b + \\Delta \\mathbf{w}$.  \n",
    "5. **Move to next block** $b+1$.  \n",
    "\n",
    "After all blocks are processed, you have $\\widehat{\\mathbf{W}}$, your 4-bit (or otherwise quantized) weights.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Arbitrary Order Insight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The OBQ method selects weights (parameters in a model) for quantization in a certain order, determined by which will add the least additional error\n",
    "- GPTQ observes that for large models, quantizing weights in any fixed order can perform just as well.\n",
    "\n",
    " GPTQ aims to quantize all weights in the same order for all rows of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Lazy Batch-Updates\n",
    "\n",
    "This scheme won’t be fast because it requires updating a huge matrix with very few computations for each entry. This type of operation can’t utilize the full compute capabilities of GPUs and will be slowed down by memory limitations: *To resolve this, GPTQ introduces “lazy batch” updates. It turns out that the final rounding decisions for a given column are only affected by updates performed on that column, not on later columns.*\n",
    "\n",
    "**Therefore, GPTQ can apply the algorithm to a batch of columns at a time (like 128 columns), updating only those columns and a corresponding block of the matrix.**\n",
    "\n",
    "## Step 3. Cholesky Reformulation\n",
    "\n",
    "When the algorithm scales up to very large models, numerical inaccuracies can become a problem. Specifically, repeated applications of a certain operation can accumulate numerical errors.\n",
    "\n",
    "To tackle this, GPTQ uses a Cholesky decomposition, a numerically stable method for solving certain mathematical problems. It involves precomputing some required information from the matrix using the Cholesky method. This approach, combined with a slight “dampening” (adding a small constant to diagonal elements of the matrix), helps the algorithm to avoid numerical issues.\n",
    "\n",
    "1. The GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse (a matrix that helps decide how to adjust the weights)\n",
    "\n",
    "2. It then runs in loops, handling batches of columns at a time.\n",
    "\n",
    "3. For each column in a batch, it quantizes the weights, calculates the error, and updates the weights in the block accordingly.\n",
    "\n",
    "4. After processing the batch, it updates all remaining weights based on the block’s errors.\n",
    "\n",
    "### Results\n",
    "\n",
    "The GPTQ algorithm was tested on various language generation tasks. It was compared with other quantization methods, like rounding all weights to the **nearest quantized value (RTN).**\n",
    "\n",
    "# Quantize an LLM with GPTQModel\n",
    "\n",
    "The transformers library with bitsandbytes allows you to quantize a model when it’s loaded using the load_in_4bit=true argument, which requires downloading full models and storing them in your RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base model and output directory\n",
    "model_id = \"gpt2\"\n",
    "out_dir = model_id + \"-GPTQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quantize config, model and tokenizer\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    damp_percent=0.01,\n",
    "    desc_act=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`group_size`:\n",
    "- **Meaning**: “Group size” typically refers to how weights (e.g., within a matrix or layer) are grouped for quantization. Often, GPTQ or related methods do \"per-channel\" or \"per-group\" quantization, meaning the quantization parameters (like scale and offset) can vary across groups.\n",
    "- **Why groups?**: Having a single scaling factor for an entire large weight matrix can be too coarse and might degrade accuracy. But having per-weight scaling factors is impractical. A middle ground is to partition the matrix into “groups,” each with its own quantization parameters.\n",
    "- `group_size=128`: This means that within each weight matrix, we divide it into groups (for instance, 128 consecutive weights in a row or column, depending on the implementation) and quantize each group with its own scale/offset. This typically improves the quantization fidelity compared to one global scale factor, while remaining computationally feasible and memory-efficient.\n",
    "\n",
    "`damp_percent`:\n",
    "- Meaning: This parameter refers to a “damping” factor, often used in second-order or GPTQ-style quantization algorithms that approximate or utilize the Hessian (or some second-order information).\n",
    "- In practice: “Damping” helps stabilize the Hessian-based updates, preventing large swings in weight adjustments. Sometimes it’s expressed as a percentage of the trace or a fraction that controls how strong the regularization/damping is on the Hessian or the updates.\n",
    "- `0.01` means a small damping, so we slightly regularize the second-order approximations when computing how to quantize or adjust weights.\n",
    "\n",
    "`desc_act`:\n",
    "- Meaning: It allows you to process rows based on decreasing activation, meaning the most important or impactful rows (determined by sampled inputs and outputs) are processed first. This method aims to place most of the quantization error (inevitably introduced during quantization) on less significant weights. **However, when used alongside group size, desc_act can lead to performance slowdowns due to the need to frequently reload quantization parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/r_quantization/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantization process **relies heavily on samples** to evaluate and enhance the quality of the quantization. They provide a means of comparison between the outputs produced by the original and the newly quantized model. \n",
    "\n",
    "In the context of this article, we utilize the C4 (Colossal Clean Crawled Corpus) dataset to generate our samples. The C4 dataset is a large-scale, multilingual collection of web text gathered from the Common Crawl project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 356318 examples [00:03, 113592.38 examples/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2441065 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Load data and tokenize examples\n",
    "n_samples = 1024\n",
    "data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n",
    "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'timestamp', 'url'],\n",
       "    num_rows: 5120\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'At Seven Avenue Design we design projects in wide range of scales from one space to whole neighborhoods.\\nAt Seven Avenue Design we transform the interior of your space to make it functional, personal, according to your space and budget.\\nOur architectural services include your conceptual and schematic design listening to your objectives, space requirements and even what you are planning ahead.\\nWe accompany you to make your design a palpable building.',\n",
       " 'timestamp': datetime.datetime(2019, 4, 25, 19, 52, 44),\n",
       " 'url': 'http://sevenavedesign.com/studio/'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2441065])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format tokenized examples\n",
    "examples_ids = []\n",
    "for _ in range(n_samples):\n",
    "    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
    "    j = i + tokenizer.model_max_length\n",
    "    input_ids = tokenized_data.input_ids[:, i:j]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[5419,  284, 9494,  ..., 7486,   11, 4055]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - triton is not installed, reset use_triton to False\n",
      "INFO - Start quantizing layer 1/12\n",
      "INFO - Quantizing attn.c_attn in layer 1/12...\n",
      "INFO - Quantizing attn.c_proj in layer 1/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 1/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 1/12...\n",
      "INFO - Start quantizing layer 2/12\n",
      "INFO - Quantizing attn.c_attn in layer 2/12...\n",
      "INFO - Quantizing attn.c_proj in layer 2/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 2/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 2/12...\n",
      "INFO - Start quantizing layer 3/12\n",
      "INFO - Quantizing attn.c_attn in layer 3/12...\n",
      "INFO - Quantizing attn.c_proj in layer 3/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 3/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 3/12...\n",
      "INFO - Start quantizing layer 4/12\n",
      "INFO - Quantizing attn.c_attn in layer 4/12...\n",
      "INFO - Quantizing attn.c_proj in layer 4/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 4/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 4/12...\n",
      "INFO - Start quantizing layer 5/12\n",
      "INFO - Quantizing attn.c_attn in layer 5/12...\n",
      "INFO - Quantizing attn.c_proj in layer 5/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 5/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 5/12...\n",
      "INFO - Start quantizing layer 6/12\n",
      "INFO - Quantizing attn.c_attn in layer 6/12...\n",
      "INFO - Quantizing attn.c_proj in layer 6/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 6/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 6/12...\n",
      "INFO - Start quantizing layer 7/12\n",
      "INFO - Quantizing attn.c_attn in layer 7/12...\n",
      "INFO - Quantizing attn.c_proj in layer 7/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 7/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 7/12...\n",
      "INFO - Start quantizing layer 8/12\n",
      "INFO - Quantizing attn.c_attn in layer 8/12...\n",
      "INFO - Quantizing attn.c_proj in layer 8/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 8/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 8/12...\n",
      "INFO - Start quantizing layer 9/12\n",
      "INFO - Quantizing attn.c_attn in layer 9/12...\n",
      "INFO - Quantizing attn.c_proj in layer 9/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 9/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 9/12...\n",
      "INFO - Start quantizing layer 10/12\n",
      "INFO - Quantizing attn.c_attn in layer 10/12...\n",
      "INFO - Quantizing attn.c_proj in layer 10/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 10/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 10/12...\n",
      "INFO - Start quantizing layer 11/12\n",
      "INFO - Quantizing attn.c_attn in layer 11/12...\n",
      "INFO - Quantizing attn.c_proj in layer 11/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 11/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 11/12...\n",
      "INFO - Start quantizing layer 12/12\n",
      "INFO - Quantizing attn.c_attn in layer 12/12...\n",
      "INFO - Quantizing attn.c_proj in layer 12/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 12/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 12/12...\n"
     ]
    }
   ],
   "source": [
    "# Quantize with GPTQ\n",
    "model.quantize(\n",
    "    examples_ids,\n",
    "    batch_size=1,\n",
    "    use_triton=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt2-GPTQ'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2-GPTQ/tokenizer_config.json',\n",
       " 'gpt2-GPTQ/special_tokens_map.json',\n",
       " 'gpt2-GPTQ/vocab.json',\n",
       " 'gpt2-GPTQ/merges.txt',\n",
       " 'gpt2-GPTQ/added_tokens.json',\n",
       " 'gpt2-GPTQ/tokenizer.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_quantized(out_dir, use_safetensors=True)\n",
    "tokenizer.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Triton is not installed, reset use_triton to False.\n",
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Reload model and tokenizer\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    out_dir,\n",
    "    device=device,\n",
    "    use_triton=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s check that the model is working correctly. The AutoGPTQ model (mostly) works as a normal transformers model, which makes it compatible with inference pipelines, as shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'GPT2GPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream to make every single American in America a better place,\" she said.\n",
      "\n",
      "\"When the dream happens, I will work hard to make that dream happen,\" she said. \"I will stand up for my daughter, my son\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "result = generator(\"I have a dream\", do_sample=True, max_length=50)[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A more in-depth evaluation would require measuring the perplexity of the quantized model versus the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, text):\n",
    "    # Encode the text\n",
    "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Define input_ids and target_ids\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # Loss calculation\n",
    "    neg_log_likelihood = outputs.loss\n",
    "\n",
    "    # Perplexity calculation\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/r_quantization/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'GPT2GPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream of becoming President of the United States.\"\n",
      "\n",
      "While the Republicans are not exactly happy—and that's the point, probably—they are clearly not backing down on the fact that Trump has been \"disciplined.\" I think\n"
     ]
    }
   ],
   "source": [
    "original_generator = pipeline('text-generation', model=original_model, tokenizer=original_tokenizer)\n",
    "result = original_generator(\"I have a dream\", do_sample=True, max_length=50)[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"I have a dream\"\n",
    "ppl_quant     = calculate_perplexity(model, original_text)\n",
    "ppl_original = calculate_perplexity(original_model, original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original perplexity:  68.75\n",
      "Quantized perplexity:    81.62\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original perplexity:  {ppl_original.item():.2f}\")\n",
    "print(f\"Quantized perplexity:    {ppl_quant.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_quantization",
   "language": "python",
   "name": "kr_quantization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
