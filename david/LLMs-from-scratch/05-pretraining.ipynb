{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers\n",
    "- Computing the training and validation set losses to assess the quality of LLM-generated text during training\n",
    "- Implementing a training function and pretraining the LLM\n",
    "- Saving and loading model weights to continue training an LLM\n",
    "- Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.3\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1\n",
      "tensorflow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scripts.previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use dropout of 0.1 above, but it's relatively common to train LLMs without dropout nowadays\n",
    "- Modern LLMs also don't use bias vectors in the `nn.Linear` layers for the query, key, and value matrices (unlike earlier GPT models), which is achieved by setting `\"qkv_bias\": False`\n",
    "- We reduce the context length (`context_length`) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens\n",
    "  - This is so that more readers will be able to follow and execute the code examples on their laptop computer\n",
    "  - However, please feel free to increase the `context_length` to 1024 tokens (this would not require any code changes)\n",
    "  - We will also load a model with a 1024 `context_length` later from pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trf_blocks[11].att.mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from scripts.previous_chapters import generate_text_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see above, the model does not produce good text because it has not been trained yet\n",
    "- How do we measure or capture what \"good text\" is, in a numeric form, to track it during training?\n",
    "- The next subsection introduces metrics to calculate a loss metric for the generated outputs that we can use to measure the training progress\n",
    "- The next chapters on finetuning LLMs will also introduce additional ways to measure model quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss: Cross Entropy and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434, 17853,\n",
       "          5308,  3398, 13174, 43071]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(probas, dim=-1) # Check that probabilities sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "# Greedy\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How fare the outputs are to targets ?\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-index.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The token probabilities corresponding to the target indices are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16833,  3626,  6100],\n",
       "        [   40,  1107,   588]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3626,  6100,   345],\n",
       "        [ 1107,   588, 11311]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "# indices of the target tokens in the vocabulary\n",
    "idxs = targets[text_idx]\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], idxs]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "idxs1 = targets[text_idx]\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], idxs1]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to maximize all these values, bringing them close to a probability of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself; this is out of the scope of this book, but I have recorded a lecture with more details here: [L8.2 Logistic Regression Loss Function](https://www.youtube.com/watch?v=GxJe0DZvydM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take the average log prob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal is to make this average log probability as large as possible by optimizing the model weights\n",
    "- Due to the log, the largest possible value is 0, and we are currently far away from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In deep learning, instead of maximizing the average log-probability, it's a standard convention **to minimize the *negative* average log-probability value**; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0\n",
    "- **The value negative of -10.7722, i.e., 10.7722, is also called cross-entropy loss in deep learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch already implements a `cross_entropy` function that carries out the previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the `cross_entropy` function in PyTorch, we want to flatten these tensors by combining them over the batch dimension. We will ignore the batching and assume everything is from the same sequence length L: [L, Vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape) # size: [L, vocab_size]\n",
    "print(\"Flattened targets:\", targets_flat.shape) # size: [L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3626,  6100,   345,  1107,   588, 11311])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_logit_target_token = []\n",
    "l_probas_target_token = []\n",
    "for idx, target in enumerate(targets_flat):\n",
    "    # Get the logit\n",
    "    logits_token = logits_flat[idx]\n",
    "    logit_target_token = logits_token[target]\n",
    "    l_logit_target_token.append(logit_target_token)\n",
    "    # Get probas\n",
    "    probas_token = torch.softmax(logits_token, dim=-1)\n",
    "    probas_target_token = probas_token[target]\n",
    "    l_probas_target_token.append(probas_target_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate in a single tensor\n",
    "tensor_probas_target_token = torch.tensor(l_probas_target_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.4541e-05, 3.1061e-05, 1.1563e-05, 1.0337e-05, 5.6776e-05, 4.7559e-06])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_probas_target_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss = -torch.mean(torch.log(tensor_probas_target_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7940)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A concept related to the cross-entropy loss is the perplexity of an LLM\n",
    "- The perplexity is simply the exponential of the cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 48,725 words or tokens)\n",
    "- In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset\n",
    "- Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20479"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With 5,145 tokens, the text is very short for training an LLM, but again, it's for educational purposes (we will also load pretrained weights later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we divide the dataset into a training and a validation set and use the data loaders from chapter 2 to prepare the batches for LLM training\n",
    "- For visualization purposes, the figure below assumes a `max_length=6`, but for the training loader, we set the `max_length` equal to the context length that the LLM supports\n",
    "- The figure below only shows the input tokens for simplicity\n",
    "    - Since we train the LLM to predict the next word in the text, the targets look the same as these inputs, except that the targets are shifted by one position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/batching.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.previous_chapters import create_dataloader_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 256\n"
     ]
    }
   ],
   "source": [
    "max_length = GPT_CONFIG_124M[\"context_length\"]\n",
    "print(\"Max length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride: 256\n"
     ]
    }
   ],
   "source": [
    "stride = GPT_CONFIG_124M[\"context_length\"]\n",
    "print(\"Stride:\", stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  518,     6, 14707,   588,   257,  2156,   286,  4116,    13,   679])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    6, 14707,   588,   257,  2156,   286,  4116,    13,   679,  1422])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to calculate the loss in a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we implement a utility function to calculate the cross-entropy loss of a given batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    # Remember that GPTDatasetV1 class returns input and target in the getitem method\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We focus on a simple training function (if you are interested in augmenting this training function with more advanced techniques, such as learning rate warmup, cosine annealing, and gradient clipping, please refer to [Appendix D](../../appendix-D/01_main-chapter-code))\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/train-steps.webp\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.817, Val loss 9.928\n",
      "Ep 1 (Step 000005): Train loss 8.065, Val loss 8.336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:05,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.622, Val loss 7.052\n",
      "Ep 2 (Step 000015): Train loss 6.047, Val loss 6.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:04,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
      "Ep 3 (Step 000020): Train loss 5.587, Val loss 6.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:01<00:03,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3 (Step 000025): Train loss 5.535, Val loss 6.404\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Ep 4 (Step 000030): Train loss 5.157, Val loss 6.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:02<00:03,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 (Step 000035): Train loss 4.991, Val loss 6.385\n",
      "Every effort moves you a a so a a a. Gisburn, and a. Gisburn, and a, and a.            \"I the of the of the picture and he had been. I\n",
      "Ep 5 (Step 000040): Train loss 4.366, Val loss 6.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:02<00:02,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you, I had been a--as of the--as of the of the of the, I had been--and it's had been, in the of the of the picture, as a of the of the of the man of the of the of\n",
      "Ep 6 (Step 000045): Train loss 4.017, Val loss 6.201\n",
      "Ep 6 (Step 000050): Train loss 3.511, Val loss 6.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:03<00:02,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know the    \"I looked--I had a little.             \"I, and I had been the donkey.            \n",
      "Ep 7 (Step 000055): Train loss 3.530, Val loss 6.175\n",
      "Ep 7 (Step 000060): Train loss 2.720, Val loss 6.129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:03<00:01,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know the picture to see the picture.                    \"I he was his pictures-c.             \n",
      "Ep 8 (Step 000065): Train loss 2.279, Val loss 6.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:04<00:01,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8 (Step 000070): Train loss 1.936, Val loss 6.217\n",
      "Every effort moves you know,\" was not that the picture.  \"I had the last word. Gisburn's an!  \"Oh, in the moment--as Jack himself, as he was his own the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.559, Val loss 6.220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:04<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9 (Step 000080): Train loss 1.229, Val loss 6.245\n",
      "Every effort moves you know,\" was not that my hostess was \"interesting\": on that Mrs. \"Yes--and by me to me to have to see a smile behind his close that he had married her--the quality of Jack's \"strongest,\" she was\n",
      "Ep 10 (Step 000085): Train loss 0.940, Val loss 6.315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"     \"I didn't face that he had married her--the quality of the a fashionable painter--and by holding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.817434310913086,\n",
       " 8.065202045440675,\n",
       " 6.622488212585449,\n",
       " 6.047133350372315,\n",
       " 5.58669261932373,\n",
       " 5.5352483749389645,\n",
       " 5.157100677490234,\n",
       " 4.99132661819458,\n",
       " 4.366218757629395,\n",
       " 4.0169103145599365,\n",
       " 3.5108954906463623,\n",
       " 3.530308485031128,\n",
       " 2.7198591232299805,\n",
       " 2.279480051994324,\n",
       " 1.9362924098968506,\n",
       " 1.5586584329605102,\n",
       " 1.2285195589065552,\n",
       " 0.9399571537971496]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.927745819091797,\n",
       " 8.335558891296387,\n",
       " 7.051747798919678,\n",
       " 6.600673198699951,\n",
       " 6.47714376449585,\n",
       " 6.4035162925720215,\n",
       " 6.38599157333374,\n",
       " 6.384969711303711,\n",
       " 6.264615535736084,\n",
       " 6.2005839347839355,\n",
       " 6.147343635559082,\n",
       " 6.175339221954346,\n",
       " 6.129348278045654,\n",
       " 6.142926216125488,\n",
       " 6.217245101928711,\n",
       " 6.220163345336914,\n",
       " 6.245031356811523,\n",
       " 6.3148016929626465]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at the results above, we can see that the model starts out generating incomprehensible strings of words, whereas towards the end, it's able to produce grammatically more or less correct sentences\n",
    "- However, based on the training and validation set losses, we can see that the model starts overfitting\n",
    "- If we were to check a few passages it writes towards the end, we would find that they are contained in the training set verbatim -- it simply memorizes the training data\n",
    "- **Later, we will cover decoding strategies that can mitigate this memorization by a certain degree**\n",
    "- Note that the overfitting here occurs because **we have a very, very small training set**, and we iterate over it so many times\n",
    "  - The LLM training here primarily serves educational purposes; we mainly want to see that the model can learn to produce coherent text\n",
    "  - Instead of spending weeks or months on training this model on vast amounts of expensive hardware, we load pretrained weights later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_llmfs",
   "language": "python",
   "name": "kr_llmfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
